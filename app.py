import re
import streamlit as st
from PIL import Image
import pandas as pd
import io
import json
import unicodedata
import os
# [å…³é”®æ›´æ–°] å¯¼å…¥æ–°çš„ä¾§è¾¹æ ç»„ä»¶
from streamlit_option_menu import option_menu

# --- SDK ä¾èµ– ---
# requirements.txt needs to include: alibabacloud_ocr_api20210707, pandas, streamlit, pillow, openpyxl, streamlit-option-menu
try:
    from alibabacloud_ocr_api20210707.client import Client as OcrClient
    from alibabacloud_tea_openapi import models as open_api_models
    from alibabacloud_ocr_api20210707 import models as ocr_models
    ALIYUN_SDK_AVAILABLE = True
except ImportError:
    ALIYUN_SDK_AVAILABLE = False


# --- MOCK FUNCTION for Report Analyzer ---
def analyze_reports_ultimate(file_paths):
    """
    ä¸€ä¸ªæ¨¡æ‹Ÿå‡½æ•°ï¼Œç”¨äºæ›¿ä»£ç¼ºå¤±çš„ analyze_excel.py æ¨¡å—ã€‚
    å®ƒä¼šç”Ÿæˆä¸€äº›ç¤ºä¾‹åˆ†æç»“æœã€‚
    """
    summaries = []
    unknown_codes = {"XYZ": 2, "ABC": 5} 

    for path in file_paths:
        file_name = os.path.basename(path)
        summary = f"### æ–‡ä»¶ '{file_name}' çš„åˆ†ææ‘˜è¦:\n"
        summary += f"- **æ€»æˆ¿é—´æ•°**: {len(path) * 5 % 50 + 10}\n"
        summary += f"- **æ€»äººæ•°**: {len(path) * 8 % 80 + 20}\n"
        summary += "- **å…³é”®å‘ç°**: æ­¤æŠ¥å‘Šä¸­çš„ VIP å®¢äººæ•°é‡è¾ƒå¤šã€‚\n---"
        summaries.append(summary)

    return summaries, unknown_codes


# ==============================================================================
# --- APP 1: OCR é”€å”®é€šçŸ¥ç”Ÿæˆå™¨ ---
# ==============================================================================
def run_ocr_app():
    """Contains all logic and UI for the OCR Sales Notification Generator."""

    # --- é…ç½®ä¿¡æ¯ ---
    TEAM_TYPE_MAP = { "CON": "ä¼šè®®å›¢", "FIT": "æ•£å®¢å›¢", "WA": "å©šå®´å›¢" }
    DEFAULT_TEAM_TYPE = "æ—…æ¸¸å›¢"
    ALL_ROOM_CODES = [
        "DETN", "DKN", "DQN", "DSKN", "DSTN", "DTN", "EKN", "EKS", "ESN", "ESS",
        "ETN", "ETS", "FSN", "FSB", "FSC", "OTN", "PSA", "PSB", "RSN", "SKN",
        "SQN", "SQS", "SSN", "SSS", "STN", "STS", "JDEN", "JDKN", "JDKS", "JEKN",
        "JESN", "JESS", "JETN", "JETS", "JKN", "JLKN", "JTN", "JTS", "PSC", "PSD",
        "VCKN", "VCKD", "SITN", "JEN", "JIS", "JTIN"
    ]

    # --- ç™»å½•æ£€æŸ¥å‡½æ•° ---
    def check_password():
        """è¿”å› True å¦‚æœç”¨æˆ·å·²ç™»å½•, å¦åˆ™è¿”å› False."""
        def login_form():
            with st.form("Credentials"):
                st.text_input("ç”¨æˆ·å", key="username")
                st.text_input("å¯†ç ", type="password", key="password")
                st.form_submit_button("ç™»å½•", on_click=password_entered)

        def password_entered():
            app_username = os.environ.get("APP_USERNAME")
            app_password = os.environ.get("APP_PASSWORD")
            
            if st.session_state["username"] == app_username and st.session_state["password"] == app_password:
                st.session_state["password_correct"] = True
                del st.session_state["password"]
                del st.session_state["username"]
            else:
                st.session_state["password_correct"] = False

        if not os.environ.get("APP_USERNAME") or not os.environ.get("APP_PASSWORD"):
            st.error("é”™è¯¯ï¼šåº”ç”¨çš„ç”¨æˆ·åå’Œå¯†ç æœªåœ¨ Render çš„ç¯å¢ƒå˜é‡ä¸­æ­£ç¡®é…ç½®ã€‚è¯·å‚è€ƒæŒ‡å—æ“ä½œã€‚")
            return False

        if st.session_state.get("password_correct", False):
            return True

        login_form()
        if "password_correct" in st.session_state and not st.session_state.password_correct:
            st.error("ğŸ˜• ç”¨æˆ·åæˆ–å¯†ç ä¸æ­£ç¡®ã€‚")
        return False

    # --- OCR å¼•æ“å‡½æ•° (é˜¿é‡Œäº‘ç‰ˆ) ---
    def get_ocr_text_from_aliyun(image: Image.Image) -> str:
        if not ALIYUN_SDK_AVAILABLE:
            st.error("é”™è¯¯ï¼šé˜¿é‡Œäº‘ SDK æœªå®‰è£…ã€‚è¯·ç¡®ä¿ requirements.txt æ–‡ä»¶é…ç½®æ­£ç¡®ã€‚")
            return None
        
        access_key_id = os.environ.get("ALIYUN_ACCESS_KEY_ID")
        access_key_secret = os.environ.get("ALIYUN_ACCESS_KEY_SECRET")

        if not access_key_id or not access_key_secret:
            st.error("é”™è¯¯ï¼šé˜¿é‡Œäº‘å‡­è¯æœªåœ¨ Render çš„ç¯å¢ƒå˜é‡ä¸­é…ç½®ã€‚")
            return None
            
        try:
            config = open_api_models.Config(
                access_key_id=access_key_id,
                access_key_secret=access_key_secret,
                endpoint='ocr-api.cn-hangzhou.aliyuncs.com'
            )
            client = OcrClient(config)

            buffered = io.BytesIO()
            if image.mode == 'RGBA':
                image = image.convert('RGB')
            
            image_format = "JPEG"
            image.save(buffered, format=image_format)
            buffered.seek(0)
            
            request = ocr_models.RecognizeGeneralRequest(body=buffered)
            response = client.recognize_general(request)

            if response.status_code == 200 and response.body and response.body.data:
                data = json.loads(response.body.data)
                return data.get('content', '')
            else:
                error_message = 'æ— è¯¦ç»†ä¿¡æ¯'
                if response.body and hasattr(response.body, 'message'):
                   error_message = response.body.message
                raise Exception(f"é˜¿é‡Œäº‘ OCR API è¿”å›é”™è¯¯: {error_message}")

        except Exception as e:
            st.error(f"è°ƒç”¨é˜¿é‡Œäº‘ OCR API å¤±è´¥: {e}")
            return None

    # --- ä¿¡æ¯æå–ä¸æ ¼å¼åŒ– ---
    def extract_booking_info(ocr_text: str):
        team_name_pattern = re.compile(r'((?:CON|FIT|WA)\d+\s*/\s*[\u4e00-\u9fa5\w]+)', re.IGNORECASE)
        date_pattern = re.compile(r'(\d{1,2}/\d{1,2})')

        team_name_match = team_name_pattern.search(ocr_text)
        if not team_name_match: return "é”™è¯¯ï¼šæ— æ³•è¯†åˆ«å‡ºå›¢é˜Ÿåç§°ã€‚"
        team_name = re.sub(r'\s*/\s*', '/', team_name_match.group(1).strip())

        all_dates = date_pattern.findall(ocr_text)
        unique_dates = sorted(list(set(all_dates)))
        if not unique_dates: return "é”™è¯¯ï¼šæ— æ³•è¯†åˆ«å‡ºæœ‰æ•ˆçš„æ—¥æœŸã€‚"
        arrival_date = unique_dates[0]
        departure_date = unique_dates[-1]

        room_codes_pattern_str = '|'.join(ALL_ROOM_CODES)
        room_finder_pattern = re.compile(f'({room_codes_pattern_str})\\s*(\\d+)', re.IGNORECASE)
        price_finder_pattern = re.compile(r'\b(\d+\.\d{2})\b')

        found_rooms = [(m.group(1).upper(), int(m.group(2)), m.span()) for m in room_finder_pattern.finditer(ocr_text)]
        found_prices = [(float(m.group(1)), m.span()) for m in price_finder_pattern.finditer(ocr_text)]

        room_details = []
        available_prices = list(found_prices)

        for room_type, num_rooms, room_span in found_rooms:
            best_price = None
            best_price_index = -1
            min_distance = float('inf')

            for i, (price_val, price_span) in enumerate(available_prices):
                if price_span[0] > room_span[1]:
                    distance = price_span[0] - room_span[1]
                    if distance < min_distance:
                        min_distance = distance
                        best_price = price_val
                        best_price_index = i
            
            if best_price is not None and best_price > 0:
                room_details.append((room_type, num_rooms, int(best_price)))
                if best_price_index != -1:
                    available_prices.pop(best_price_index)

        if not room_details:
            return f"æç¤ºï¼šæ‰¾åˆ°äº†å›¢é˜Ÿ {team_name}ï¼Œä½†æœªèƒ½è‡ªåŠ¨åŒ¹é…ä»»ä½•æœ‰æ•ˆçš„æˆ¿å‹å’Œä»·æ ¼ã€‚è¯·æ£€æŸ¥åŸå§‹æ–‡æœ¬å¹¶æ‰‹åŠ¨å¡«å†™ã€‚"

        team_prefix = team_name[:3].upper()
        team_type = TEAM_TYPE_MAP.get(team_prefix, DEFAULT_TEAM_TYPE)
        room_details.sort(key=lambda x: x[1])

        try:
            arr_month, arr_day = map(int, arrival_date.split('/'))
            dep_month, dep_day = map(int, departure_date.split('/'))
            formatted_arrival = f"{arr_month}æœˆ{arr_day}æ—¥"
            formatted_departure = f"{dep_month}æœˆ{dep_day}æ—¥"
        except (ValueError, IndexError):
            return "é”™è¯¯ï¼šæ—¥æœŸæ ¼å¼æ— æ³•è§£æã€‚"

        df = pd.DataFrame(room_details, columns=['æˆ¿å‹', 'æˆ¿æ•°', 'å®šä»·'])
        return {"team_name": team_name, "team_type": team_type, "arrival_date": formatted_arrival, "departure_date": formatted_departure, "room_dataframe": df}

    def format_notification_speech(team_name, team_type, arrival_date, departure_date, room_df):
        date_range_string = f"{arrival_date}è‡³{departure_date}"
        room_details = room_df.to_dict('records')
        formatted_rooms = [f"{item['æˆ¿æ•°']}é—´{item['æˆ¿å‹']}({item['å®šä»·']})" for item in room_details]
        room_string = " ".join(formatted_rooms) if formatted_rooms else "æ— æˆ¿é—´è¯¦æƒ…"
        return f"æ–°å¢{team_type} {team_name} {date_range_string} {room_string}ã€‚é”€å”®é€šçŸ¥"

    # --- Streamlit ä¸»åº”ç”¨ ---
    st.title("é‡‘é™µå¯Œå£«åº·ä¸€ç§’boom - OCR å·¥å…·")

    if check_password():
        st.markdown("""
        **å…¨æ–°å·¥ä½œæµ**ï¼š
        1.  **ä¸Šä¼ å›¾ç‰‡ï¼Œç‚¹å‡»æå–**ï¼šç¨‹åºå°†è°ƒç”¨é˜¿é‡Œäº‘ OCR å¹¶å°†**åŸå§‹è¯†åˆ«æ–‡æœ¬**æ˜¾ç¤ºåœ¨ä¸‹æ–¹ã€‚
        2.  **è‡ªåŠ¨å¡«å……ä¸äººå·¥ä¿®æ­£**ï¼šç¨‹åºä¼šå°è¯•è‡ªåŠ¨å¡«å……ç»“æ„åŒ–ä¿¡æ¯ã€‚æ‚¨å¯ä»¥**å‚ç…§åŸå§‹æ–‡æœ¬**ï¼Œç›´æ¥åœ¨è¡¨æ ¼ä¸­ä¿®æ”¹ï¼Œç¡®ä¿ä¿¡æ¯å®Œå…¨å‡†ç¡®ã€‚
        3.  **ç”Ÿæˆè¯æœ¯**ï¼šç¡®è®¤æ— è¯¯åï¼Œç”Ÿæˆæœ€ç»ˆè¯æœ¯ã€‚
        """)

        uploaded_file = st.file_uploader("ä¸Šä¼ å›¾ç‰‡æ–‡ä»¶", type=["png", "jpg", "jpeg", "bmp"], key="ocr_uploader")

        if uploaded_file is not None:
            image = Image.open(uploaded_file)
            st.image(image, caption="ä¸Šä¼ çš„å›¾ç‰‡", width=300)

            if st.button("ä»å›¾ç‰‡æå–ä¿¡æ¯ (é˜¿é‡Œäº‘ OCR)"):
                for key in ['raw_ocr_text', 'booking_info']:
                    if key in st.session_state:
                        del st.session_state[key]
                
                with st.spinner('æ­£åœ¨è°ƒç”¨é˜¿é‡Œäº‘ OCR API è¯†åˆ«ä¸­...'):
                    ocr_text = get_ocr_text_from_aliyun(image)
                    if ocr_text:
                        st.session_state['raw_ocr_text'] = ocr_text
                        result = extract_booking_info(ocr_text)
                        if isinstance(result, str):
                            st.warning(f"è‡ªåŠ¨è§£ææç¤ºï¼š{result}")
                            st.info("è¯·å‚è€ƒä¸‹æ–¹è¯†åˆ«å‡ºçš„åŸå§‹æ–‡æœ¬ï¼Œæ‰‹åŠ¨å¡«å†™ä¿¡æ¯ã€‚")
                            empty_df = pd.DataFrame(columns=['æˆ¿å‹', 'æˆ¿æ•°', 'å®šä»·'])
                            st.session_state['booking_info'] = { "team_name": "", "team_type": DEFAULT_TEAM_TYPE, "arrival_date": "", "departure_date": "", "room_dataframe": empty_df }
                        else:
                            st.session_state['booking_info'] = result
                            st.success("ä¿¡æ¯æå–æˆåŠŸï¼è¯·åœ¨ä¸‹æ–¹æ ¸å¯¹å¹¶ç¼–è¾‘ã€‚")

        if 'booking_info' in st.session_state:
            info = st.session_state['booking_info']
            if 'raw_ocr_text' in st.session_state:
                st.markdown("---")
                st.subheader("åŸå§‹è¯†åˆ«ç»“æœ (ä¾›å‚è€ƒ)")
                st.text_area("æ‚¨å¯ä»¥ä»è¿™é‡Œå¤åˆ¶å†…å®¹æ¥ä¿®æ­£ä¸‹é¢çš„è¡¨æ ¼", st.session_state['raw_ocr_text'], height=200)
            st.markdown("---")
            st.subheader("æ ¸å¯¹ä¸ç¼–è¾‘ä¿¡æ¯")
            col1, col2, col3, col4 = st.columns(4)
            with col1: info['team_name'] = st.text_input("å›¢é˜Ÿåç§°", value=info['team_name'])
            with col2: info['team_type'] = st.selectbox("å›¢é˜Ÿç±»å‹", options=list(TEAM_TYPE_MAP.values()) + [DEFAULT_TEAM_TYPE], index=(list(TEAM_TYPE_MAP.values()) + [DEFAULT_TEAM_TYPE]).index(info['team_type']))
            with col3: arrival = st.text_input("åˆ°è¾¾æ—¥æœŸ", value=info['arrival_date'])
            with col4: departure = st.text_input("ç¦»å¼€æ—¥æœŸ", value=info['departure_date'])
            st.markdown("##### æˆ¿é—´è¯¦æƒ… (å¯ç›´æ¥åœ¨è¡¨æ ¼ä¸­ç¼–è¾‘)")
            edited_df = st.data_editor(info['room_dataframe'], num_rows="dynamic", use_container_width=True)
            if st.button("ç”Ÿæˆæœ€ç»ˆè¯æœ¯"):
                final_speech = format_notification_speech(info['team_name'], info['team_type'], arrival, departure, edited_df)
                st.subheader("ç”ŸæˆæˆåŠŸï¼")
                st.success(final_speech)
                st.code(final_speech, language=None)

# ==============================================================================
# --- APP 2: å¤šç»´å®¡æ ¸æ¯”å¯¹å¹³å° ---
# ==============================================================================
def run_comparison_app():
    """Contains all logic and UI for the Data Comparison Platform."""
    SESSION_DEFAULTS = {
        'df1': None, 'df2': None, 'df1_name': "", 'df2_name': "",
        'ran_comparison': False, 'common_rows': pd.DataFrame(),
        'matched_df': pd.DataFrame(), 'in_file1_only': pd.DataFrame(),
        'in_file2_only': pd.DataFrame(), 'compare_cols_keys': []
    }
    for key, value in SESSION_DEFAULTS.items():
        if key not in st.session_state:
            st.session_state[key] = value

    def forensic_clean_text(text):
        if not isinstance(text, str): return text
        try:
            cleaned_text = unicodedata.normalize('NFKC', text)
        except (TypeError, ValueError):
            return text
        cleaned_text = re.sub(r'[\u200B-\u200D\uFEFF\s\xa0]+', '', cleaned_text)
        return cleaned_text.strip()

    def process_and_standardize(df, mapping, case_insensitive=False, room_type_equivalents=None):
        if not mapping.get('name'):
            return pd.DataFrame()

        standard_df = pd.DataFrame()
        for col_key, col_name in mapping.items():
            if col_name and col_name in df.columns:
                standard_df[col_key] = df[col_name]

        def robust_date_parser(series):
            def process_date(date_str):
                if pd.isna(date_str): return pd.NaT
                date_str = str(date_str).strip()
                if re.match(r'^\d{1,2}/\d{1,2}', date_str):
                    date_part = date_str.split(' ')[0]
                    return f"2025-{date_part.replace('/', '-')}"
                return date_str
            return pd.to_datetime(series.apply(process_date), errors='coerce').dt.strftime('%Y-%m-%d')

        if 'start_date' in standard_df.columns:
            standard_df['start_date'] = robust_date_parser(standard_df['start_date'])
        if 'end_date' in standard_df.columns:
            standard_df['end_date'] = robust_date_parser(standard_df['end_date'])

        if 'room_type' in standard_df.columns:
            standard_df['room_type'] = standard_df['room_type'].astype(str).apply(forensic_clean_text)
            if room_type_equivalents:
                cleaned_equivalents = {forensic_clean_text(k): [forensic_clean_text(val) for val in v] for k, v in room_type_equivalents.items()}
                reverse_map = {val: key for key, values in cleaned_equivalents.items() for val in values}
                standard_df['room_type'] = standard_df['room_type'].replace(reverse_map)

        if 'price' in standard_df.columns:
            standard_df['price'] = pd.to_numeric(standard_df['price'].astype(str).str.strip(), errors='coerce')

        standard_df['name'] = standard_df['name'].astype(str).str.split(r'[ã€,ï¼Œ/]')
        standard_df = standard_df.explode('name')
        standard_df['name'] = standard_df['name'].apply(forensic_clean_text)

        if case_insensitive:
            standard_df['name'] = standard_df['name'].str.lower()

        standard_df = standard_df[standard_df['name'] != ''].dropna(subset=['name']).reset_index(drop=True)
        return standard_df

    def highlight_diff(row, col1, col2):
        style = 'background-color: #FFC7CE'
        if row.get(col1) != row.get(col2) and not (pd.isna(row.get(col1)) and pd.isna(row.get(col2))):
            return [style] * len(row)
        return [''] * len(row)

    st.title("é‡‘é™µå¯Œå£«åº·ä¸€ç§’boom - æ¯”å¯¹å¹³å°")
    st.info("å…¨æ–°æ¨¡å¼ï¼šç»“æœä»¥ç‹¬ç«‹çš„æ ‡ç­¾é¡µå±•ç¤ºï¼Œå¹¶å†…ç½®æ™ºèƒ½æ—¥æœŸç»Ÿä¸€å¼•æ“ï¼Œæ¯”å¯¹æ›´ç²¾å‡†ï¼")

    st.header("ç¬¬ 1 æ­¥: ä¸Šä¼ æ–‡ä»¶")
    if st.button("æ¸…ç©ºå¹¶é‡ç½®"):
        for key in SESSION_DEFAULTS.keys():
            if key in st.session_state:
                del st.session_state[key]
        st.rerun()

    col1, col2 = st.columns(2)
    with col1:
        uploaded_file1 = st.file_uploader("ä¸Šä¼ åå•æ–‡ä»¶ 1", type=['csv', 'xlsx'], key="comp_uploader1")
        if uploaded_file1:
            st.session_state.df1 = pd.read_excel(uploaded_file1) if uploaded_file1.name.endswith('xlsx') else pd.read_csv(uploaded_file1)
            st.session_state.df1_name = uploaded_file1.name
    with col2:
        uploaded_file2 = st.file_uploader("ä¸Šä¼ åå•æ–‡ä»¶ 2", type=['csv', 'xlsx'], key="comp_uploader2")
        if uploaded_file2:
            st.session_state.df2 = pd.read_excel(uploaded_file2) if uploaded_file2.name.endswith('xlsx') else pd.read_csv(uploaded_file2)
            st.session_state.df2_name = uploaded_file2.name

    if st.session_state.df1 is not None and st.session_state.df2 is not None:
        st.header("ç¬¬ 2 æ­¥: é€‰æ‹©è¦æ¯”å¯¹çš„åˆ— (å§“åå¿…é€‰)")
        mapping = {'file1': {}, 'file2': {}}
        cols_to_map = ['name', 'start_date', 'end_date', 'room_type', 'price']
        col_names_zh = ['å§“å', 'å…¥ä½æ—¥æœŸ', 'ç¦»å¼€æ—¥æœŸ', 'æˆ¿å‹', 'æˆ¿ä»·']

        cols1, cols2 = st.columns(2)
        with cols1:
            st.subheader(f"æ–‡ä»¶ 1: {st.session_state.df1_name}")
            df1_cols = [None] + list(st.session_state.df1.columns)
            for key, name_zh in zip(cols_to_map, col_names_zh):
                mapping['file1'][key] = st.selectbox(f"{name_zh}", df1_cols, key=f'f1_{key}')
        with cols2:
            st.subheader(f"æ–‡ä»¶ 2: {st.session_state.df2_name}")
            df2_cols = [None] + list(st.session_state.df2.columns)
            for key, name_zh in zip(cols_to_map, col_names_zh):
                mapping['file2'][key] = st.selectbox(f"{name_zh}", df2_cols, key=f'f2_{key}')

        st.header("ç¬¬ 3 æ­¥: é…ç½®ä¸æ‰§è¡Œ")
        room_type_equivalents = {}
        if mapping['file1'].get('room_type') and mapping['file2'].get('room_type'):
            with st.expander("é«˜çº§åŠŸèƒ½ï¼šç»Ÿä¸€ä¸åŒåç§°çš„æˆ¿å‹ (ä¾‹å¦‚ï¼šè®©'å¤§åºŠæˆ¿'='King Room')"):
                unique_rooms1 = st.session_state.df1[mapping['file1']['room_type']].dropna().astype(str).unique()
                unique_rooms2 = list(st.session_state.df2[mapping['file2']['room_type']].dropna().astype(str).unique())
                for room1 in unique_rooms1:
                    room_type_equivalents[room1] = st.multiselect(f"æ–‡ä»¶1çš„â€œ{room1}â€ç­‰åŒäº:", unique_rooms2, key=f"map_{room1}")

        case_insensitive = st.checkbox("æ¯”å¯¹å§“åæ—¶å¿½ç•¥å¤§å°å†™/å…¨åŠè§’", True)

        if st.button("å¼€å§‹æ¯”å¯¹", type="primary"):
            if not mapping['file1'].get('name') or not mapping['file2'].get('name'):
                st.error("è¯·ç¡®ä¿ä¸¤è¾¹æ–‡ä»¶çš„â€œå§“åâ€éƒ½å·²æ­£ç¡®é€‰æ‹©ã€‚")
            else:
                with st.spinner('æ­£åœ¨æ‰§è¡Œç»ˆææ¯”å¯¹...'):
                    st.session_state.ran_comparison = True
                    st.session_state.df1.sort_values(by=mapping['file1']['name'], inplace=True, ignore_index=True)
                    st.session_state.df2.sort_values(by=mapping['file2']['name'], inplace=True, ignore_index=True)
                    std_df1 = process_and_standardize(st.session_state.df1, mapping['file1'], case_insensitive, room_type_equivalents)
                    std_df2 = process_and_standardize(st.session_state.df2, mapping['file2'], case_insensitive)
                    merged_df = pd.merge(std_df1, std_df2, on='name', how='outer', suffixes=('_1', '_2'))
                    cols1_for_check = [f"{c}_1" for c in std_df1.columns if c != 'name']
                    cols2_for_check = [f"{c}_2" for c in std_df2.columns if c != 'name']
                    both_exist_mask = merged_df[cols1_for_check].notna().any(axis=1) & merged_df[cols2_for_check].notna().any(axis=1)
                    st.session_state.common_rows = merged_df[both_exist_mask].copy().reset_index(drop=True)
                    only_in_1_mask = merged_df[cols1_for_check].notna().any(axis=1) & merged_df[cols2_for_check].isna().all(axis=1)
                    st.session_state.in_file1_only = merged_df[only_in_1_mask].reset_index(drop=True)
                    only_in_2_mask = merged_df[cols1_for_check].isna().all(axis=1) & merged_df[cols2_for_check].notna().any(axis=1)
                    st.session_state.in_file2_only = merged_df[only_in_2_mask].reset_index(drop=True)
                    st.session_state.compare_cols_keys = [key for key in ['start_date', 'end_date', 'room_type', 'price'] if mapping['file1'].get(key) and mapping['file2'].get(key)]
                    if not st.session_state.common_rows.empty and st.session_state.compare_cols_keys:
                        condition = pd.Series(True, index=st.session_state.common_rows.index)
                        for key in st.session_state.compare_cols_keys:
                            condition &= (st.session_state.common_rows[f'{key}_1'] == st.session_state.common_rows[f'{key}_2']) | \
                                         (st.session_state.common_rows[f'{key}_1'].isna() & st.session_state.common_rows[f'{key}_2'].isna())
                        st.session_state.matched_df = st.session_state.common_rows[condition]
                    else:
                        st.session_state.matched_df = st.session_state.common_rows

        if st.session_state.ran_comparison:
            st.header("ç¬¬ 4 æ­¥: æŸ¥çœ‹æ¯”å¯¹ç»“æœ")
            tab_list = ["ç»“æœæ€»è§ˆ"]
            tab_name_map = {'start_date': "å…¥ä½æ—¥æœŸ", 'end_date': "ç¦»å¼€æ—¥æœŸ", 'room_type': "æˆ¿å‹", 'price': "æˆ¿ä»·"}
            for key in st.session_state.compare_cols_keys:
                tab_list.append(tab_name_map[key])
            tabs = st.tabs(tab_list)

            with tabs[0]:
                st.subheader("å®è§‚ç»Ÿè®¡")
                stat_cols = st.columns(3)
                matched_count = len(st.session_state.matched_df)
                only_1_count = len(st.session_state.in_file1_only)
                only_2_count = len(st.session_state.in_file2_only)
                stat_cols[0].metric("ä¿¡æ¯å®Œå…¨ä¸€è‡´", matched_count)
                stat_cols[1].metric(f"ä»… '{st.session_state.df1_name}' æœ‰", only_1_count)
                stat_cols[2].metric(f"ä»… '{st.session_state.df2_name}' æœ‰", only_2_count)

                st.subheader("äººå‘˜åå•è¯¦æƒ…")
                with st.expander(f"æŸ¥çœ‹ {matched_count} æ¡ä¿¡æ¯å®Œå…¨ä¸€è‡´çš„åå•"):
                    if not st.session_state.matched_df.empty:
                        st.dataframe(st.session_state.matched_df[['name']].rename(columns={'name': 'å§“å'}))
                    else:
                        st.write("æ²¡æœ‰ä¿¡æ¯å®Œå…¨ä¸€è‡´çš„äººå‘˜ã€‚")

                with st.expander(f"æŸ¥çœ‹ {only_1_count} æ¡ä»…å­˜åœ¨äº '{st.session_state.df1_name}' çš„åå•"):
                    if not st.session_state.in_file1_only.empty:
                        display_cols_1 = ['name'] + [c for c in cols_to_map if f"{c}_1" in st.session_state.in_file1_only.columns]
                        display_df_1 = st.session_state.in_file1_only[[f"{c}_1" if c != 'name' else 'name' for c in display_cols_1]]
                        display_df_1.columns = [col_names_zh[cols_to_map.index(c)] if c != 'name' else 'å§“å' for c in display_cols_1]
                        st.dataframe(display_df_1)
                    else:
                        st.write("æ²¡æœ‰äººå‘˜ã€‚")

                with st.expander(f"æŸ¥çœ‹ {only_2_count} æ¡ä»…å­˜åœ¨äº '{st.session_state.df2_name}' çš„åå•"):
                    if not st.session_state.in_file2_only.empty:
                        display_cols_2 = ['name'] + [c for c in cols_to_map if f"{c}_2" in st.session_state.in_file2_only.columns]
                        display_df_2 = st.session_state.in_file2_only[[f"{c}_2" if c != 'name' else 'name' for c in display_cols_2]]
                        display_df_2.columns = [col_names_zh[cols_to_map.index(c)] if c != 'name' else 'å§“å' for c in display_cols_2]
                        st.dataframe(display_df_2)
                    else:
                        st.write("æ²¡æœ‰äººå‘˜ã€‚")

            for i, key in enumerate(st.session_state.compare_cols_keys):
                with tabs[i+1]:
                    col1_name, col2_name = f'{key}_1', f'{key}_2'
                    display_name = col_names_zh[cols_to_map.index(key)]
                    st.subheader(f"ã€{display_name}ã€‘æ¯”å¯¹è¯¦æƒ…")
                    if not st.session_state.common_rows.empty:
                        compare_df = st.session_state.common_rows[['name', col1_name, col2_name]].copy()
                        compare_df.rename(columns={'name': 'å§“å', col1_name: f'æ–‡ä»¶1 - {display_name}', col2_name: f'æ–‡ä»¶2 - {display_name}'}, inplace=True)
                        styled_df = compare_df.style.apply(highlight_diff, col1=f'æ–‡ä»¶1 - {display_name}', col2=f'æ–‡ä»¶2 - {display_name}', axis=1)
                        st.dataframe(styled_df)
                    else:
                        st.info("ä¸¤ä¸ªæ–‡ä»¶ä¸­æ²¡æœ‰å…±åŒçš„äººå‘˜å¯ä¾›è¿›è¡Œç»†èŠ‚æ¯”å¯¹ã€‚")

        st.divider()
        st.header("åŸå§‹æ•°æ®é¢„è§ˆ (ç‚¹å‡»æ¯”å¯¹åä¼šæŒ‰å§“åæ’åº)")
        c1, c2 = st.columns(2)
        with c1:
            st.caption(f"æ–‡ä»¶ 1: {st.session_state.df1_name}")
            st.dataframe(st.session_state.df1)
        with c2:
            st.caption(f"æ–‡ä»¶ 2: {st.session_state.df2_name}")
            st.dataframe(st.session_state.df2)

# ==============================================================================
# --- APP 3: Excel æŠ¥å‘Šåˆ†æå™¨ ---
# ==============================================================================
def run_analyzer_app():
    """Contains all logic and UI for the Excel Report Analyzer."""
    st.title("é‡‘é™µå¯Œå£«åº·ä¸€ç§’boom - æŠ¥å‘Šåˆ†æå™¨")
    st.markdown("---ä¼¯çˆµé…’åº—å›¢é˜ŸæŠ¥è¡¨åˆ†æå·¥å…·---")

    uploaded_files = st.file_uploader("è¯·ä¸Šä¼ æ‚¨çš„ Excel æŠ¥å‘Šæ–‡ä»¶ (.xlsx)", type=["xlsx"], accept_multiple_files=True, key="analyzer_uploader")

    if uploaded_files:
        temp_dir = "./temp_uploaded_files"
        if not os.path.exists(temp_dir):
            os.makedirs(temp_dir)

        file_paths = []
        for uploaded_file in uploaded_files:
            temp_file_path = os.path.join(temp_dir, uploaded_file.name)
            with open(temp_file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            file_paths.append(temp_file_path)

        desired_order = ["æ¬¡æ—¥åˆ°è¾¾", "æ¬¡æ—¥åœ¨ä½", "æ¬¡æ—¥ç¦»åº—", "åå¤©åˆ°è¾¾"]
        def sort_key(file_path):
            file_name = os.path.basename(file_path)
            for i, keyword in enumerate(desired_order):
                if keyword in file_name:
                    return i
            return len(desired_order)
        file_paths.sort(key=sort_key)

        if st.button("å¼€å§‹åˆ†æ"):
            with st.spinner("æ­£åœ¨åˆ†æä¸­ï¼Œè¯·ç¨å€™..."):
                st.subheader("åˆ†æç»“æœ")
                summaries, unknown_codes = analyze_reports_ultimate(file_paths)
            
            for summary in summaries:
                st.markdown(summary)

            if unknown_codes:
                st.subheader("ä¾¦æµ‹åˆ°çš„æœªçŸ¥æˆ¿å‹ä»£ç  (è¯·æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°è§„åˆ™)")
                st.info("è¿™æ˜¯æ¨¡æ‹Ÿæ•°æ®ï¼Œç”¨ä»¥å±•ç¤ºåŠŸèƒ½ã€‚")
                for code, count in unknown_codes.items():
                    st.write(f"ä»£ç : '{code}' (å‡ºç°äº† {count} æ¬¡)")
            
            for f_path in file_paths:
                try:
                    os.remove(f_path)
                except OSError as e:
                    st.warning(f"Error removing temp file {f_path}: {e}")
            try:
                os.rmdir(temp_dir)
            except OSError as e:
                st.warning(f"Error removing temp directory {temp_dir}: {e}")
    else:
        st.info("è¯·ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ª Excel æ–‡ä»¶ä»¥å¼€å§‹åˆ†æã€‚")

    st.markdown("""
    ---
    #### ä½¿ç”¨è¯´æ˜ï¼š
    1. ç‚¹å‡» "Browse files" ä¸Šä¼ æ‚¨çš„ Excel æŠ¥å‘Šã€‚å¯ä»¥åŒæ—¶ä¸Šä¼ å¤šä¸ªæ–‡ä»¶ã€‚
    2. æ–‡ä»¶ä¸Šä¼ åï¼Œç‚¹å‡» "å¼€å§‹åˆ†æ" æŒ‰é’®ã€‚
    3. åˆ†æç»“æœå°†æ˜¾ç¤ºåœ¨ä¸‹æ–¹ã€‚
    """)


# ==============================================================================
# --- Main App Router ---
# ==============================================================================
st.set_page_config(layout="wide", page_title="é‡‘é™µå¯Œå£«åº·ä¸€ç§’boom")

# [å…³é”®æ›´æ–°] ä½¿ç”¨æ–°çš„ option_menu ç»„ä»¶
with st.sidebar:
    app_choice = option_menu(
        menu_title="é‡‘é™µå¯Œå£«åº·ä¸€ç§’boom",  # required
        options=["OCR å·¥å…·", "æ¯”å¯¹å¹³å°", "æŠ¥å‘Šåˆ†æå™¨"],  # required
        icons=["camera-reels", "columns-gap", "file-earmark-bar-graph"],  # optional
        menu_icon="cast",  # optional
        default_index=0,  # optional
    )

st.sidebar.markdown("---")
st.sidebar.info("è¿™æ˜¯ä¸€ä¸ªå°†å¤šä¸ªå·¥å…·é›†æˆåˆ°ä¸€èµ·çš„åº”ç”¨ã€‚")

if app_choice == "OCR å·¥å…·":
    run_ocr_app()
elif app_choice == "æ¯”å¯¹å¹³å°":
    run_comparison_app()
elif app_choice == "æŠ¥å‘Šåˆ†æå™¨":
    run_analyzer_app()

