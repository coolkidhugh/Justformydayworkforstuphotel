import re
import streamlit as st
from PIL import Image
import pandas as pd
import io
import json
import unicodedata
import os
import traceback
from datetime import timedelta, date
# [å…³é”®æ›´æ–°] å¯¼å…¥æ–°çš„ä¾§è¾¹æ ç»„ä»¶
from streamlit_option_menu import option_menu

# --- SDK ä¾èµ– ---
# requirements.txt needs to include: alibabacloud_ocr_api20210707, pandas, streamlit, pillow, openpyxl, streamlit-option-menu
try:
    from alibabacloud_ocr_api20210707.client import Client as OcrClient
    from alibabacloud_tea_openapi import models as open_api_models
    from alibabacloud_ocr_api20210707 import models as ocr_models
    from alibabacloud_tea_util import models as util_models
    ALIYUN_SDK_AVAILABLE = True
except ImportError:
    ALIYUN_SDK_AVAILABLE = False


# --- MOCK FUNCTION for Report Analyzer ---
def analyze_reports_ultimate(file_paths):
    """
    ä¸€ä¸ªæ¨¡æ‹Ÿå‡½æ•°ï¼Œç”¨äºæ›¿ä»£ç¼ºå¤±çš„ analyze_excel.py æ¨¡å—ã€‚
    å®ƒä¼šç”Ÿæˆä¸€äº›ç¤ºä¾‹åˆ†æç»“æœï¼Œæ¨¡ä»¿ç”¨æˆ·æœŸæœ›çš„è¾“å‡ºæ ¼å¼ã€‚
    """
    # Based on user's image: 5b25a5b0e1df25073f860126ea39cca3.png
    summaries = [
        "ã€æ¬¡æ—¥åœ¨ä½ã€‘ï¼šæœ‰æ•ˆæ€»æˆ¿æ•° 64 é—´(å…± 59 äºº)ï¼Œå…¶ä¸­ä¼šè®®/å…¬å¸å›¢é˜Ÿ(MGM/MTC)[5ä¸ªå›¢é˜Ÿï¼Œå…±23é—´]åˆ†å¸ƒ: é‡‘é™µæ¥¼ 17 é—´, äºšå¤ªæ¥¼ 6 é—´ã€‚(æ— GTOæ—…è¡Œç¤¾æˆ¿)ã€‚",
        "ã€æ¬¡æ—¥ç¦»åº—ã€‘ï¼šæœ‰æ•ˆæ€»æˆ¿æ•° 240 é—´(å…± 251 äºº)ï¼Œå…¶ä¸­ä¼šè®®/å…¬å¸å›¢é˜Ÿ(MGM/MTC)[9ä¸ªå›¢é˜Ÿï¼Œå…±232é—´]åˆ†å¸ƒ: é‡‘é™µæ¥¼ 173 é—´, äºšå¤ªæ¥¼ 58 é—´, å…¶ä»–æ¥¼ 1 é—´ã€‚æ—…è¡Œç¤¾(GTO)æˆ¿[2ä¸ªå›¢é˜Ÿ, 8é—´, å…±12äºº]åˆ†å¸ƒ: é‡‘é™µæ¥¼ 8 é—´, äºšå¤ªæ¥¼ 0 é—´ã€‚",
        "ã€æ¬¡æ—¥åˆ°åº—ã€‘ï¼šæœ‰æ•ˆæ€»æˆ¿æ•° 46 é—´(å…± 37 äºº)ï¼Œå…¶ä¸­ä¼šè®®/å…¬å¸å›¢é˜Ÿ(MGM/MTC)[8ä¸ªå›¢é˜Ÿ, å…±17é—´]åˆ†å¸ƒ: é‡‘é™µæ¥¼ 1 é—´, äºšå¤ªæ¥¼ 6 é—´ã€‚(æ— GTOæ—…è¡Œç¤¾æˆ¿)ã€‚",
        "ã€åå¤©åˆ°åº—ã€‘ï¼šæœ‰æ•ˆæ€»æˆ¿æ•° 0 é—´(å…± 0 äºº)ï¼Œ(æ— ä¼šè®®/å…¬å¸å›¢é˜Ÿæˆ¿)ï¼Œ(æ— GTOæ—…è¡Œç¤¾æˆ¿)ã€‚"
    ]
    # The mock function can return a static result as the core logic is missing
    unknown_codes = {"PSA": 1}
    return summaries, unknown_codes


# ==============================================================================
# --- APP 1: OCR å·¥å…· (V6 - ä¸‰æ­¥å®¡æ ¸æµç¨‹) ---
# ==============================================================================
def run_ocr_app_detailed():
    """Contains all logic and UI for the Detailed OCR Sales Notification Generator."""

    # --- é…ç½®ä¿¡æ¯ ---
    TEAM_TYPE_MAP = { "CON": "ä¼šè®®å›¢", "FIT": "æ•£å®¢å›¢", "WA": "å©šå®´å›¢" }
    DEFAULT_TEAM_TYPE = "æ—…æ¸¸å›¢"
    SALES_LIST = ["é™ˆæ´ªè´", "å€ªæ–Œ", "åˆ˜äºšç‚œ", "é»„å©·", "è’‹æ€æº", "é»„æ³½æµ©", "è’‹å…‰èª", "å´çš“å®‡", "æ½˜èŒœ", "æŸæ–¹"]
    ALL_ROOM_CODES = [
        "DETN", "DKN", "DQN", "DQS", "DSKN", "DSTN", "DTN", "EKN", "EKS", "ESN", "ESS",
        "ETN", "ETS", "FSN", "FSB", "FSC", "OTN", "PSA", "PSB", "RSN", "SKN",
        "SQN", "SQS", "SSN", "SSS", "STN", "STS", "JDEN", "JDKN", "JDKS", "JEKN",
        "JESN", "JESS", "JETN", "JETS", "JKN", "JLKN", "JTN", "JTS", "PSC", "PSD",
        "VCKN", "VCKD", "SITN", "JEN", "JIS", "JTIN", "SON", "DON"
    ]

    # --- OCR å¼•æ“å‡½æ•° ---
    def get_ocr_data_from_aliyun(image: Image.Image):
        if not ALIYUN_SDK_AVAILABLE:
            st.error("é”™è¯¯ï¼šé˜¿é‡Œäº‘ SDK æœªå®‰è£…ã€‚")
            return None
        if "aliyun_credentials" not in st.secrets:
            st.error("é”™è¯¯ï¼šé˜¿é‡Œäº‘å‡­è¯æœªåœ¨ Secrets ä¸­é…ç½®ã€‚")
            return None
        access_key_id = st.secrets.aliyun_credentials.get("access_key_id")
        access_key_secret = st.secrets.aliyun_credentials.get("access_key_secret")
        if not access_key_id or not access_key_secret:
            st.error("é”™è¯¯ï¼šé˜¿é‡Œäº‘ AccessKey ID æˆ– Secret æœªåœ¨ Secrets ä¸­æ­£ç¡®é…ç½®ã€‚")
            return None
        try:
            config = open_api_models.Config(access_key_id=access_key_id, access_key_secret=access_key_secret, endpoint='ocr-api.cn-hangzhou.aliyuncs.com')
            client = OcrClient(config)
            buffered = io.BytesIO()
            if image.mode == 'RGBA': image = image.convert('RGB')
            image.save(buffered, format="JPEG")
            buffered.seek(0)
            # [å…³é”®ä¿®æ­£] ä½¿ç”¨é€šç”¨æ–‡å­—è¯†åˆ«æ¥å£ï¼Œç¨³å®šæ€§æ›´å¥½
            request = ocr_models.RecognizeGeneralRequest(body=buffered)
            response = client.recognize_general(request)
            if response.status_code == 200 and response.body and response.body.data:
                return json.loads(response.body.data)
            else:
                error_message = 'æ— è¯¦ç»†ä¿¡æ¯'
                if response.body and hasattr(response.body, 'message'): error_message = response.body.message
                raise Exception(f"é˜¿é‡Œäº‘ OCR API è¿”å›é”™è¯¯: {error_message}")
        except Exception as e:
            st.error(f"è°ƒç”¨é˜¿é‡Œäº‘ OCR API å¤±è´¥: {e}")
            return None

    # --- ä»çº¯æ–‡æœ¬ä¸­æå–ä¿¡æ¯ ---
    def extract_booking_info_from_text(ocr_text: str):
        if not ocr_text: return "é”™è¯¯ï¼šæ–‡æœ¬å†…å®¹ä¸ºç©ºã€‚"
        team_name_pattern = re.compile(r'((?:CON|FIT|WA)\d+\s*/\s*[\u4e00-\u9fa5\w]+)', re.IGNORECASE)
        team_name_match = team_name_pattern.search(ocr_text)
        if not team_name_match: return "é”™è¯¯ï¼šæ— æ³•ä»æ–‡æœ¬ä¸­è¯†åˆ«å‡ºå›¢é˜Ÿåç§°ã€‚"
        team_name = re.sub(r'\s*/\s*', '/', team_name_match.group(1).strip())
        team_prefix = team_name[:3].upper()
        team_type = TEAM_TYPE_MAP.get(team_prefix, DEFAULT_TEAM_TYPE)
        
        # åŒ¹é…æ‰€æœ‰çŠ¶æ€ä¸ºRçš„è¡Œ
        line_pattern = re.compile(
            r'^\s*R\s+'                                  # è¡Œå¿…é¡»ä»¥ R å¼€å¤´
            r'.*?'                                       # ä¸­é—´ä»»æ„å­—ç¬¦
            r'\b(' + '|'.join(ALL_ROOM_CODES) + r')\b'   # (ç»„1) æˆ¿å‹
            r'\s+(\d+)\s+'                               # (ç»„2) æˆ¿æ•°
            r'.*?'                                       # ä»»æ„å­—ç¬¦
            r'(\d{1,2}/\d{2})'                           # (ç»„3) åˆ°è¾¾æ—¥æœŸ
            r'.*?'                                       # ä»»æ„å­—ç¬¦
            r'(\d{1,2}/\d{2})'                           # (ç»„4) ç¦»å¼€æ—¥æœŸ
            r'.*?'                                       # ä»»æ„å­—ç¬¦
            r'(\d+\.\d{2})'                              # (ç»„5) ä»·æ ¼
            , re.IGNORECASE | re.MULTILINE)
            
        matches = line_pattern.findall(ocr_text)
        if not matches: return "é”™è¯¯ï¼šæœªèƒ½ä»æ–‡æœ¬ä¸­æ‰¾åˆ°ä»»ä½•çŠ¶æ€ä¸º'R'çš„æœ‰æ•ˆ'æˆ¿å‹-æˆ¿æ•°-æ—¥æœŸ-ä»·æ ¼'ç»„åˆã€‚"

        all_rows = [{"æˆ¿å‹": m[0].upper(), "æˆ¿æ•°": int(m[1]), "å®šä»·": int(float(m[4])), "arrival_raw": m[2], "departure_raw": m[3]} for m in matches]
        df = pd.DataFrame(all_rows)
        grouped = df.groupby(['arrival_raw', 'departure_raw'])
        result_groups = [{"arrival_raw": arr, "departure_raw": dep, "dataframe": gdf[['æˆ¿å‹', 'æˆ¿æ•°', 'å®šä»·']].reset_index(drop=True)} for (arr, dep), gdf in grouped]
        return {"team_name": team_name, "team_type": team_type, "booking_groups": sorted(result_groups, key=lambda x: x['arrival_raw'])}

    # --- è¯æœ¯ç”Ÿæˆ ---
    def format_notification_speech(team_name, team_type, booking_groups, salesperson):
        def format_date_range(arr_str, dep_str):
            try:
                arr_month, arr_day = arr_str.split('/')
                dep_month, dep_day = dep_str.split('/')
                if arr_month == dep_month: return f"{int(arr_month)}.{int(arr_day)}-{int(dep_day)}"
                return f"{int(arr_month)}.{int(arr_day)}-{int(dep_month)}.{int(dep_day)}"
            except: return f"{arr_str}-{dep_str}"
        speech_parts = []
        for group in booking_groups:
            date_range_string = format_date_range(group['arrival_raw'], group['departure_raw'])
            sorted_df = group['dataframe'].sort_values(by='æˆ¿æ•°', ascending=True)
            rooms_list = [f"{row['æˆ¿æ•°']}{row['æˆ¿å‹']}({row['å®šä»·']})" for _, row in sorted_df.iterrows()]
            speech_parts.append(f"{date_range_string} {''.join(rooms_list)}")
        return f"æ–°å¢{team_type} {team_name} {' '.join(speech_parts)} {salesperson}é”€å”®é€šçŸ¥"

    # --- Streamlit ä¸»åº”ç”¨ ---
    st.title("é‡‘é™µå·¥å…·ç®± - OCR å·¥å…·")
    
    if 'ocr_step' not in st.session_state:
        st.session_state.ocr_step = 0 

    uploaded_file = st.file_uploader("ä¸Šä¼ å›¾ç‰‡æ–‡ä»¶", type=["png", "jpg", "jpeg", "bmp"], key="ocr_uploader_detailed")

    if uploaded_file is not None and st.session_state.ocr_step == 0:
        st.session_state.uploaded_image_bytes = uploaded_file.getvalue()
        if st.button("1. ä»å›¾ç‰‡æå–æ–‡æœ¬"):
            with st.spinner('æ­£åœ¨è°ƒç”¨é˜¿é‡Œäº‘ OCR API...'):
                ocr_data = get_ocr_data_from_aliyun(Image.open(io.BytesIO(st.session_state.uploaded_image_bytes)))
                if ocr_data and ocr_data.get('content'):
                    st.session_state.raw_ocr_text = ocr_data.get('content')
                    st.session_state.ocr_step = 1
                    st.success("æ–‡æœ¬æå–æˆåŠŸï¼è¯·åœ¨ä¸‹æ–¹æ ¸å¯¹å¹¶ç¼–è¾‘ã€‚")
                else:
                    st.error("OCR è¯†åˆ«å¤±è´¥æˆ–æœªèƒ½è¿”å›ä»»ä½•æ–‡æœ¬å†…å®¹ã€‚")
                    st.session_state.ocr_step = 0
    
    if st.session_state.ocr_step >= 1:
        st.subheader("ç¬¬ 1 æ­¥ï¼šå®¡æ ¸å¹¶ç¼–è¾‘è¯†åˆ«çš„åŸå§‹æ–‡æœ¬")
        if 'uploaded_image_bytes' in st.session_state and st.session_state.uploaded_image_bytes:
            st.image(st.session_state.uploaded_image_bytes, use_container_width=True)
        
        edited_text = st.text_area(
            "æ‚¨å¯ä»¥ç›´æ¥åœ¨æ­¤å¤„ä¿®æ”¹è¯†åˆ«ç»“æœï¼Œç¡®ä¿æ¯æ¡è®°å½•å ä¸€è¡Œï¼Œç„¶åç‚¹å‡»è§£ææŒ‰é’®ï¼š",
            value=st.session_state.get('raw_ocr_text', ''),
            height=250
        )
        st.session_state.edited_ocr_text = edited_text

        if st.button("2. ä»æ–‡æœ¬è§£æè¡¨æ ¼"):
            result = extract_booking_info_from_text(st.session_state.edited_ocr_text)
            if isinstance(result, str):
                st.error(result)
            else:
                st.session_state.booking_info = result
                st.session_state.ocr_step = 2
                st.success("æ–‡æœ¬è§£ææˆåŠŸï¼è¯·åœ¨ä¸‹æ–¹å®¡æ ¸æœ€ç»ˆçš„ç»“æ„åŒ–è¡¨æ ¼ã€‚")

    if st.session_state.ocr_step >= 2:
        st.markdown("---")
        st.subheader("ç¬¬ 2 æ­¥ï¼šå®¡æ ¸ç»“æ„åŒ–è¡¨æ ¼")
        info = st.session_state.booking_info
        
        info['team_name'] = st.text_input("å›¢é˜Ÿåç§°", value=info.get('team_name', ''), key="team_name_final")
        
        for i, group in enumerate(info.get('booking_groups', [])):
            st.markdown(f"#### æ—¥æœŸç»„ {i+1}")
            col1, col2 = st.columns(2)
            with col1: info['booking_groups'][i]['arrival_raw'] = st.text_input("åˆ°è¾¾æ—¥æœŸ", value=group['arrival_raw'], key=f"arrival_{i}")
            with col2: info['booking_groups'][i]['departure_raw'] = st.text_input("ç¦»å¼€æ—¥æœŸ", value=group['departure_raw'], key=f"departure_{i}")

            edited_df = st.data_editor(group['dataframe'], key=f"editor_{i}", num_rows="dynamic", use_container_width=True,
                column_config={"æˆ¿æ•°": st.column_config.NumberColumn(required=True), "å®šä»·": st.column_config.NumberColumn(required=True)})
            info['booking_groups'][i]['dataframe'] = edited_df
        
        st.markdown("---")
        st.subheader("ç¬¬ 3 æ­¥ï¼šç”Ÿæˆæœ€ç»ˆè¯æœ¯")
        selected_salesperson = st.selectbox("é€‰æ‹©å¯¹åº”é”€å”®", options=SALES_LIST)

        if st.button("ç”Ÿæˆæœ€ç»ˆè¯æœ¯"):
            final_speech = format_notification_speech(info['team_name'], info['team_type'], info['booking_groups'], selected_salesperson)
            st.subheader("ç”ŸæˆæˆåŠŸï¼")
            st.success(final_speech)
            st.code(final_speech, language=None)

    if st.session_state.ocr_step > 0:
        if st.button("è¿”å›å¹¶ä¸Šä¼ æ–°å›¾ç‰‡"):
            for key in ['ocr_step', 'booking_info', 'raw_ocr_text', 'edited_ocr_text', 'uploaded_image_bytes']:
                if key in st.session_state: del st.session_state[key]
            st.rerun()


# ==============================================================================
# --- APP 2: å¤šç»´å®¡æ ¸æ¯”å¯¹å¹³å° ---
# ==============================================================================
def run_comparison_app():
    """Contains all logic and UI for the Data Comparison Platform."""
    SESSION_DEFAULTS = {
        'df1': None, 'df2': None, 'df1_name': "", 'df2_name': "",
        'ran_comparison': False, 'common_rows': pd.DataFrame(),
        'matched_df': pd.DataFrame(), 'in_file1_only': pd.DataFrame(),
        'in_file2_only': pd.DataFrame(), 'compare_cols_keys': []
    }
    for key, value in SESSION_DEFAULTS.items():
        if key not in st.session_state:
            st.session_state[key] = value

    def forensic_clean_text(text):
        if not isinstance(text, str): return text
        try:
            cleaned_text = unicodedata.normalize('NFKC', text)
        except (TypeError, ValueError):
            return text
        cleaned_text = re.sub(r'[\u200B-\u200D\uFEFF\s\xa0]+', '', cleaned_text)
        return cleaned_text.strip()

    def process_and_standardize(df, mapping, case_insensitive=False, room_type_equivalents=None):
        if not mapping.get('name'):
            return pd.DataFrame()

        standard_df = pd.DataFrame()
        for col_key, col_name in mapping.items():
            if col_name and col_name in df.columns:
                standard_df[col_key] = df[col_name]

        def robust_date_parser(series):
            def process_date(date_str):
                if pd.isna(date_str): return pd.NaT
                date_str = str(date_str).strip()
                if re.match(r'^\d{1,2}/\d{1,2}', date_str):
                    date_part = date_str.split(' ')[0]
                    return f"2025-{date_part.replace('/', '-')}"
                return date_str
            return pd.to_datetime(series.apply(process_date), errors='coerce').dt.strftime('%Y-%m-%d')

        if 'start_date' in standard_df.columns:
            standard_df['start_date'] = robust_date_parser(standard_df['start_date'])
        if 'end_date' in standard_df.columns:
            standard_df['end_date'] = robust_date_parser(standard_df['end_date'])

        if 'room_type' in standard_df.columns:
            standard_df['room_type'] = standard_df['room_type'].astype(str).apply(forensic_clean_text)
            if room_type_equivalents:
                direct_map = {}
                for key, values in room_type_equivalents.items():
                    for value in values:
                        direct_map[forensic_clean_text(value)] = forensic_clean_text(key)
                standard_df['room_type'] = standard_df['room_type'].replace(direct_map)

        if 'price' in standard_df.columns:
            standard_df['price'] = pd.to_numeric(standard_df['price'].astype(str).str.strip(), errors='coerce')

        standard_df['name'] = standard_df['name'].astype(str).str.split(r'[ã€,ï¼Œ/]')
        standard_df = standard_df.explode('name')
        standard_df['name'] = standard_df['name'].apply(forensic_clean_text)

        if case_insensitive:
            standard_df['name'] = standard_df['name'].str.lower()

        standard_df = standard_df[standard_df['name'] != ''].dropna(subset=['name']).reset_index(drop=True)
        return standard_df

    def highlight_diff(row, col1, col2):
        style = 'background-color: #FFC7CE'
        if row.get(col1) != row.get(col2) and not (pd.isna(row.get(col1)) and pd.isna(row.get(col2))):
            return [style] * len(row)
        return [''] * len(row)

    st.title("é‡‘é™µå·¥å…·ç®± - æ¯”å¯¹å¹³å°")
    st.info("å…¨æ–°æ¨¡å¼ï¼šç»“æœä»¥ç‹¬ç«‹çš„æ ‡ç­¾é¡µå±•ç¤ºï¼Œå¹¶å†…ç½®æ™ºèƒ½æ—¥æœŸç»Ÿä¸€å¼•æ“ï¼Œæ¯”å¯¹æ›´ç²¾å‡†ï¼")

    st.header("ç¬¬ 1 æ­¥: ä¸Šä¼ æ–‡ä»¶")
    if st.button("æ¸…ç©ºå¹¶é‡ç½®"):
        for key in SESSION_DEFAULTS.keys():
            if key in st.session_state:
                del st.session_state[key]
        st.rerun()

    col1, col2 = st.columns(2)
    with col1:
        uploaded_file1 = st.file_uploader("ä¸Šä¼ åå•æ–‡ä»¶ 1", type=['csv', 'xlsx'], key="comp_uploader1")
        if uploaded_file1:
            st.session_state.df1 = pd.read_excel(uploaded_file1) if uploaded_file1.name.endswith('xlsx') else pd.read_csv(uploaded_file1)
            st.session_state.df1_name = uploaded_file1.name
    with col2:
        uploaded_file2 = st.file_uploader("ä¸Šä¼ åå•æ–‡ä»¶ 2", type=['csv', 'xlsx'], key="comp_uploader2")
        if uploaded_file2:
            st.session_state.df2 = pd.read_excel(uploaded_file2) if uploaded_file2.name.endswith('xlsx') else pd.read_csv(uploaded_file2)
            st.session_state.df2_name = uploaded_file2.name

    if st.session_state.df1 is not None and st.session_state.df2 is not None:
        st.header("ç¬¬ 2 æ­¥: é€‰æ‹©è¦æ¯”å¯¹çš„åˆ— (å§“åå¿…é€‰)")
        mapping = {'file1': {}, 'file2': {}}
        cols_to_map = ['name', 'start_date', 'end_date', 'room_type', 'price']
        col_names_zh = ['å§“å', 'å…¥ä½æ—¥æœŸ', 'ç¦»å¼€æ—¥æœŸ', 'æˆ¿å‹', 'æˆ¿ä»·']

        cols1, cols2 = st.columns(2)
        with cols1:
            st.subheader(f"æ–‡ä»¶ 1: {st.session_state.df1_name}")
            df1_cols = [None] + list(st.session_state.df1.columns)
            for key, name_zh in zip(cols_to_map, col_names_zh):
                mapping['file1'][key] = st.selectbox(f"{name_zh}", df1_cols, key=f'f1_{key}')
        with cols2:
            st.subheader(f"æ–‡ä»¶ 2: {st.session_state.df2_name}")
            df2_cols = [None] + list(st.session_state.df2.columns)
            for key, name_zh in zip(cols_to_map, col_names_zh):
                mapping['file2'][key] = st.selectbox(f"{name_zh}", df2_cols, key=f'f2_{key}')

        st.header("ç¬¬ 3 æ­¥: é…ç½®ä¸æ‰§è¡Œ")
        room_type_equivalents = {}
        if mapping['file1'].get('room_type') and mapping['file2'].get('room_type'):
            with st.expander("é«˜çº§åŠŸèƒ½ï¼šç»Ÿä¸€ä¸åŒåç§°çš„æˆ¿å‹ (ä¾‹å¦‚ï¼šè®©'å¤§åºŠæˆ¿'='King Room')"):
                unique_rooms1 = st.session_state.df1[mapping['file1']['room_type']].dropna().astype(str).unique()
                unique_rooms2 = list(st.session_state.df2[mapping['file2']['room_type']].dropna().astype(str).unique())
                for room1 in unique_rooms1:
                    room_type_equivalents[room1] = st.multiselect(f"æ–‡ä»¶1çš„â€œ{room1}â€ç­‰åŒäº:", unique_rooms2, key=f"map_{room1}")

        case_insensitive = st.checkbox("æ¯”å¯¹å§“åæ—¶å¿½ç•¥å¤§å°å†™/å…¨åŠè§’", True)

        if st.button("å¼€å§‹æ¯”å¯¹", type="primary"):
            if not mapping['file1'].get('name') or not mapping['file2'].get('name'):
                st.error("è¯·ç¡®ä¿ä¸¤è¾¹æ–‡ä»¶çš„â€œå§“åâ€éƒ½å·²æ­£ç¡®é€‰æ‹©ã€‚")
            else:
                with st.spinner('æ­£åœ¨æ‰§è¡Œç»ˆææ¯”å¯¹...'):
                    st.session_state.ran_comparison = True
                    std_df1 = process_and_standardize(st.session_state.df1.copy(), mapping['file1'], case_insensitive)
                    std_df2 = process_and_standardize(st.session_state.df2.copy(), mapping['file2'], case_insensitive, room_type_equivalents=room_type_equivalents)
                    merged_df = pd.merge(std_df1, std_df2, on='name', how='outer', suffixes=('_1', '_2'))
                    cols1_for_check = [f"{c}_1" for c in std_df1.columns if c != 'name']
                    cols2_for_check = [f"{c}_2" for c in std_df2.columns if c != 'name']
                    both_exist_mask = merged_df[cols1_for_check].notna().any(axis=1) & merged_df[cols2_for_check].notna().any(axis=1)
                    st.session_state.common_rows = merged_df[both_exist_mask].copy().reset_index(drop=True)
                    only_in_1_mask = merged_df[cols1_for_check].notna().any(axis=1) & merged_df[cols2_for_check].isna().all(axis=1)
                    st.session_state.in_file1_only = merged_df[only_in_1_mask].reset_index(drop=True)
                    only_in_2_mask = merged_df[cols1_for_check].isna().all(axis=1) & merged_df[cols2_for_check].notna().any(axis=1)
                    st.session_state.in_file2_only = merged_df[only_in_2_mask].reset_index(drop=True)
                    st.session_state.compare_cols_keys = [key for key in ['start_date', 'end_date', 'room_type', 'price'] if mapping['file1'].get(key) and mapping['file2'].get(key)]
                    if not st.session_state.common_rows.empty and st.session_state.compare_cols_keys:
                        condition = pd.Series(True, index=st.session_state.common_rows.index)
                        for key in st.session_state.compare_cols_keys:
                            condition &= (st.session_state.common_rows[f'{key}_1'] == st.session_state.common_rows[f'{key}_2']) | \
                                         (st.session_state.common_rows[f'{key}_1'].isna() & st.session_state.common_rows[f'{key}_2'].isna())
                        st.session_state.matched_df = st.session_state.common_rows[condition]
                    else:
                        st.session_state.matched_df = st.session_state.common_rows

        if st.session_state.ran_comparison:
            st.header("ç¬¬ 4 æ­¥: æŸ¥çœ‹æ¯”å¯¹ç»“æœ")
            tab_list = ["ç»“æœæ€»è§ˆ"]
            tab_name_map = {'start_date': "å…¥ä½æ—¥æœŸ", 'end_date': "ç¦»å¼€æ—¥æœŸ", 'room_type': "æˆ¿å‹", 'price': "æˆ¿ä»·"}
            for key in st.session_state.compare_cols_keys:
                tab_list.append(tab_name_map[key])
            tabs = st.tabs(tab_list)

            with tabs[0]:
                st.subheader("å®è§‚ç»Ÿè®¡")
                stat_cols = st.columns(3)
                matched_count = len(st.session_state.matched_df)
                only_1_count = len(st.session_state.in_file1_only)
                only_2_count = len(st.session_state.in_file2_only)
                stat_cols[0].metric("ä¿¡æ¯å®Œå…¨ä¸€è‡´", matched_count)
                stat_cols[1].metric(f"ä»… '{st.session_state.df1_name}' æœ‰", only_1_count)
                stat_cols[2].metric(f"ä»… '{st.session_state.df2_name}' æœ‰", only_2_count)

                st.subheader("äººå‘˜åå•è¯¦æƒ…")
                with st.expander(f"æŸ¥çœ‹ {matched_count} æ¡ä¿¡æ¯å®Œå…¨ä¸€è‡´çš„åå•"):
                    if not st.session_state.matched_df.empty:
                        st.dataframe(st.session_state.matched_df[['name']].rename(columns={'name': 'å§“å'}))
                    else:
                        st.write("æ²¡æœ‰ä¿¡æ¯å®Œå…¨ä¸€è‡´çš„äººå‘˜ã€‚")

                with st.expander(f"æŸ¥çœ‹ {only_1_count} æ¡ä»…å­˜åœ¨äº '{st.session_state.df1_name}' çš„åå•"):
                    if not st.session_state.in_file1_only.empty:
                        display_cols_1 = ['name'] + [c for c in cols_to_map if f"{c}_1" in st.session_state.in_file1_only.columns]
                        display_df_1 = st.session_state.in_file1_only[[f"{c}_1" if c != 'name' else 'name' for c in display_cols_1]]
                        display_df_1.columns = [col_names_zh[cols_to_map.index(c)] if c != 'name' else 'å§“å' for c in display_cols_1]
                        st.dataframe(display_df_1)
                    else:
                        st.write("æ²¡æœ‰äººå‘˜ã€‚")

                with st.expander(f"æŸ¥çœ‹ {only_2_count} æ¡ä»…å­˜åœ¨äº '{st.session_state.df2_name}' çš„åå•"):
                    if not st.session_state.in_file2_only.empty:
                        display_cols_2 = ['name'] + [c for c in cols_to_map if f"{c}_2" in st.session_state.in_file2_only.columns]
                        display_df_2 = st.session_state.in_file2_only[[f"{c}_2" if c != 'name' else 'name' for c in display_cols_2]]
                        display_df_2.columns = [col_names_zh[cols_to_map.index(c)] if c != 'name' else 'å§“å' for c in display_cols_2]
                        st.dataframe(display_df_2)
                    else:
                        st.write("æ²¡æœ‰äººå‘˜ã€‚")

            for i, key in enumerate(st.session_state.compare_cols_keys):
                with tabs[i+1]:
                    col1_name, col2_name = f'{key}_1', f'{key}_2'
                    display_name = col_names_zh[cols_to_map.index(key)]
                    st.subheader(f"ã€{display_name}ã€‘æ¯”å¯¹è¯¦æƒ…")
                    if not st.session_state.common_rows.empty:
                        compare_df = st.session_state.common_rows[['name', col1_name, col2_name]].copy()
                        compare_df.rename(columns={'name': 'å§“å', col1_name: f'æ–‡ä»¶1 - {display_name}', col2_name: f'æ–‡ä»¶2 - {display_name}'}, inplace=True)
                        styled_df = compare_df.style.apply(highlight_diff, col1=f'æ–‡ä»¶1 - {display_name}', col2=f'æ–‡ä»¶2 - {display_name}', axis=1)
                        st.dataframe(styled_df)
                    else:
                        st.info("ä¸¤ä¸ªæ–‡ä»¶ä¸­æ²¡æœ‰å…±åŒçš„äººå‘˜å¯ä¾›è¿›è¡Œç»†èŠ‚æ¯”å¯¹ã€‚")

        st.divider()
        st.header("åŸå§‹æ•°æ®é¢„è§ˆ")
        c1, c2 = st.columns(2)
        with c1:
            st.caption(f"æ–‡ä»¶ 1: {st.session_state.df1_name}")
            st.dataframe(st.session_state.df1)
        with c2:
            st.caption(f"æ–‡ä»¶ 2: {st.session_state.df2_name}")
            st.dataframe(st.session_state.df2)

# ==============================================================================
# --- APP 3: å›¢é˜Ÿåˆ°åº—ç»Ÿè®¡ ---
# ==============================================================================
def run_analyzer_app():
    st.title("ğŸ“ˆ å›¢é˜Ÿåˆ°åº—ç»Ÿè®¡")
    st.markdown("---ä¼¯çˆµé…’åº—å›¢é˜ŸæŠ¥è¡¨åˆ†æå·¥å…·---")

    uploaded_files = st.file_uploader("è¯·ä¸Šä¼ æ‚¨çš„ Excel æŠ¥å‘Šæ–‡ä»¶ (.xlsx)", type=["xlsx"], accept_multiple_files=True, key="analyzer_uploader")

    if uploaded_files:
        st.subheader("åˆ†æç»“æœ")
        
        # Create a temporary directory to save uploaded files
        temp_dir = "./temp_uploaded_files"
        os.makedirs(temp_dir, exist_ok=True)

        file_paths = []
        for uploaded_file in uploaded_files:
            # Save the uploaded file to the temporary directory
            temp_file_path = os.path.join(temp_dir, uploaded_file.name)
            with open(temp_file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            file_paths.append(temp_file_path)

        # Define the desired order of keywords
        desired_order = ["æ¬¡æ—¥åˆ°è¾¾", "æ¬¡æ—¥åœ¨ä½", "æ¬¡æ—¥ç¦»åº—", "åå¤©åˆ°è¾¾"]

        # Custom sort function
        def sort_key(file_path):
            file_name = os.path.basename(file_path)
            for i, keyword in enumerate(desired_order):
                if keyword in file_name:
                    return i
            return len(desired_order) # Files without keywords go to the end

        # Sort the file_paths based on the desired order
        file_paths.sort(key=sort_key)

        if st.button("å¼€å§‹åˆ†æ"): # Use a button to trigger analysis
            with st.spinner("æ­£åœ¨åˆ†æä¸­ï¼Œè¯·ç¨å€™..."):
                summaries, unknown_codes = analyze_reports_ultimate(file_paths)
            
            for summary in summaries:
                st.write(summary)

            if unknown_codes:
                st.subheader("ä¾¦æµ‹åˆ°çš„æœªçŸ¥æˆ¿å‹ä»£ç  (è¯·æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°è§„åˆ™)")
                for code, count in unknown_codes.items():
                    st.write(f"ä»£ç : '{code}' (å‡ºç°äº† {count} æ¬¡)")
            
            # Clean up temporary files and directory
            for f_path in file_paths:
                os.remove(f_path)
            os.rmdir(temp_dir)

    else:
        st.info("è¯·ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ª Excel æ–‡ä»¶ä»¥å¼€å§‹åˆ†æã€‚")

    st.markdown("""
    --- 
    #### ä½¿ç”¨è¯´æ˜ï¼š
    1. ç‚¹å‡» "Browse files" ä¸Šä¼ æ‚¨çš„ Excel æŠ¥å‘Šã€‚å¯ä»¥åŒæ—¶ä¸Šä¼ å¤šä¸ªæ–‡ä»¶ã€‚
    2. æ–‡ä»¶ä¸Šä¼ åï¼Œç‚¹å‡» "å¼€å§‹åˆ†æ" æŒ‰é’®ã€‚
    3. åˆ†æç»“æœå°†æ˜¾ç¤ºåœ¨ä¸‹æ–¹ã€‚
    """)

# ==============================================================================
# --- [æœ€ç»ˆç‰ˆ] APP 4: é…’åº—å…¥ä½æ•°æ®åˆ†æåº”ç”¨ ---
# ==============================================================================
@st.cache_data
def process_data(uploaded_file):
    # ... (æ­¤å‡½æ•°ä¿æŒä¸å˜)
    df = pd.read_excel(uploaded_file)
    df.columns = [str(col).strip().upper() for col in df.columns]
    required_cols = ['çŠ¶æ€', 'æˆ¿ç±»', 'æˆ¿æ•°', 'åˆ°è¾¾', 'ç¦»å¼€', 'æˆ¿ä»·', 'å¸‚åœºç ']
    rename_map = {'ROOM CATEGORY': 'æˆ¿ç±»', 'ROOMS': 'æˆ¿æ•°', 'ARRIVAL': 'åˆ°è¾¾', 'DEPARTURE': 'ç¦»å¼€', 'RATE': 'æˆ¿ä»·', 'MARKET': 'å¸‚åœºç ', 'STATUS': 'çŠ¶æ€'}
    df.rename(columns=rename_map, inplace=True)
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        st.error(f"ä¸Šä¼ çš„æ–‡ä»¶ç¼ºå°‘ä»¥ä¸‹å¿…è¦çš„åˆ—: {', '.join(missing_cols)}ã€‚è¯·æ£€æŸ¥æ–‡ä»¶ã€‚")
        return None, None
    df['åˆ°è¾¾_str'] = df['åˆ°è¾¾'].astype(str).str.split(' ').str[0]
    df['ç¦»å¼€_str'] = df['ç¦»å¼€'].astype(str).str.split(' ').str[0]
    df['åˆ°è¾¾'] = pd.to_datetime(df['åˆ°è¾¾_str'], format='%y/%m/%d', errors='coerce')
    df['ç¦»å¼€'] = pd.to_datetime(df['ç¦»å¼€_str'], format='%y/%m/%d', errors='coerce')
    df['æˆ¿ä»·'] = pd.to_numeric(df['æˆ¿ä»·'], errors='coerce')
    df['æˆ¿æ•°'] = pd.to_numeric(df['æˆ¿æ•°'], errors='coerce')
    df['å¸‚åœºç '] = df['å¸‚åœºç '].astype(str)
    df.dropna(subset=['åˆ°è¾¾', 'ç¦»å¼€', 'æˆ¿ä»·', 'æˆ¿æ•°', 'æˆ¿ç±»'], inplace=True)
    df['æˆ¿æ•°'] = df['æˆ¿æ•°'].astype(int)
    jinling_rooms = ['DETN', 'DKN', 'DQN', 'DQS', 'DSKN', 'DSTN', 'DTN', 'EKN', 'EKS', 'ESN', 'ESS', 'ETN', 'ETS', 'FSB', 'FSC', 'FSN', 'OTN', 'PSA', 'PSB', 'RSN', 'SKN', 'SQN', 'SQS', 'SSN', 'SSS', 'STN', 'STS']
    yatal_rooms = ['JDEN', 'JDKN', 'JDKS', 'JEKN', 'JESN', 'JESS', 'JETN', 'JETS', 'JKN', 'JLKN', 'JTN', 'JTS', 'PSC', 'PSD', 'VCKD', 'VCKN']
    room_to_building = {code: "é‡‘é™µæ¥¼" for code in jinling_rooms}
    room_to_building.update({code: "äºšå¤ªæ¥¼" for code in yatal_rooms})
    df = df[df['æˆ¿ç±»'].isin(jinling_rooms + yatal_rooms)].copy()
    df['æ¥¼å±‚'] = df['æˆ¿ç±»'].map(room_to_building)
    df['å…¥ä½å¤©æ•°'] = (df['ç¦»å¼€'].dt.normalize() - df['åˆ°è¾¾'].dt.normalize()).dt.days
    df_for_arrivals = df.copy()
    df_for_stays = df[(df['å…¥ä½å¤©æ•°'] > 0) & (df['çŠ¶æ€'].isin(['R', 'I']))].copy()
    if df_for_stays.empty:
        return df_for_arrivals, pd.DataFrame()
    df_repeated = df_for_stays.loc[df_for_stays.index.repeat(df_for_stays['å…¥ä½å¤©æ•°'])]
    date_offset = df_repeated.groupby(level=0).cumcount()
    df_repeated['ä½åº—æ—¥'] = df_repeated['åˆ°è¾¾'].dt.normalize() + pd.to_timedelta(date_offset, unit='D')
    expanded_df = df_repeated.drop(columns=['åˆ°è¾¾', 'ç¦»å¼€', 'å…¥ä½å¤©æ•°']).reset_index(drop=True)
    return df_for_arrivals, expanded_df.copy()


def run_data_analysis_app():
    # ... (æ­¤å‡½æ•°ä¿æŒä¸å˜)
    st.title("é‡‘é™µå·¥å…·ç®± - æ•°æ®åˆ†æé©¾é©¶èˆ±")
    uploaded_file = st.file_uploader("ä¸Šä¼ æ‚¨çš„Excelæ–‡ä»¶", type=["xlsx", "xls"], key="data_analysis_uploader")
    if not uploaded_file:
        st.info("è¯·ä¸Šä¼ æ‚¨çš„Excelæ–‡ä»¶ä»¥å¼€å§‹åˆ†æã€‚")
        return
    try:
        original_df, expanded_df = process_data(uploaded_file)
        if original_df is None: return
        if original_df.empty:
            st.warning("ä¸Šä¼ çš„æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®è®°å½•ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å†…å®¹å’Œæ ¼å¼ã€‚")
            return
        st.success(f"æ–‡ä»¶ '{uploaded_file.name}' ä¸Šä¼ å¹¶å¤„ç†æˆåŠŸï¼")
        st.header("1. æ¯æ—¥åˆ°åº—/ç¦»åº—æˆ¿æ•°ç»Ÿè®¡")
        with st.expander("ç‚¹å‡»å±•å¼€æˆ–æŠ˜å ", expanded=True):
            st.subheader("åˆ°åº—æˆ¿æ•°ç»Ÿè®¡")
            all_statuses = sorted(original_df['çŠ¶æ€'].unique())
            selected_arrival_statuses = st.multiselect("é€‰æ‹©åˆ°åº—çŠ¶æ€", options=all_statuses, default=['R'])
            arrival_dates_str = st.text_input("è¾“å…¥åˆ°åº—æ—¥æœŸ (ç”¨é€—å·åˆ†éš”, æ ¼å¼: YYYY/MM/DD)", pd.to_datetime(original_df['åˆ°è¾¾'].min()).strftime('%Y/%m/%d') if not original_df.empty else "")
            arrival_summary = pd.DataFrame()
            if arrival_dates_str and selected_arrival_statuses:
                try:
                    date_strings = [d.strip() for d in arrival_dates_str.split(',') if d.strip()]
                    selected_arrival_dates = [pd.to_datetime(d, format='%Y/%m/%d').date() for d in date_strings]
                    arrival_df = original_df[(original_df['çŠ¶æ€'].isin(selected_arrival_statuses)) & (original_df['åˆ°è¾¾'].dt.date.isin(selected_arrival_dates))].copy()
                    if not arrival_df.empty:
                        arrival_summary = arrival_df.groupby([arrival_df['åˆ°è¾¾'].dt.date, 'æ¥¼å±‚'])['æˆ¿æ•°'].sum().unstack(fill_value=0)
                        arrival_summary.index.name = "åˆ°åº—æ—¥æœŸ"
                        st.dataframe(arrival_summary)
                    else:
                        st.warning(f"åœ¨æ‰€é€‰æ—¥æœŸå’ŒçŠ¶æ€å†…æ²¡æœ‰æ‰¾åˆ°åˆ°åº—è®°å½•ã€‚")
                except ValueError:
                    st.error("åˆ°åº—æ—¥æœŸæ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·è¾“å…¥ YYYY/MM/DD æ ¼å¼ã€‚")
            st.subheader("ç¦»åº—æˆ¿æ•°ç»Ÿè®¡")
            selected_departure_statuses = st.multiselect("é€‰æ‹©ç¦»åº—çŠ¶æ€", options=all_statuses, default=['R', 'S', 'I', 'O'])
            departure_dates_str = st.text_input("è¾“å…¥ç¦»åº—æ—¥æœŸ (ç”¨é€—å·åˆ†éš”, æ ¼å¼: YYYY/MM/DD)", pd.to_datetime(original_df['ç¦»å¼€'].min()).strftime('%Y/%m/%d') if not original_df.empty else "")
            departure_summary = pd.DataFrame()
            if departure_dates_str and selected_departure_statuses:
                try:
                    date_strings = [d.strip() for d in departure_dates_str.split(',') if d.strip()]
                    selected_departure_dates = [pd.to_datetime(d, format='%Y/%m/%d').date() for d in date_strings]
                    departure_df = original_df[(original_df['çŠ¶æ€'].isin(selected_departure_statuses)) & (original_df['ç¦»å¼€'].dt.date.isin(selected_departure_dates))].copy()
                    if not departure_df.empty:
                        departure_summary = departure_df.groupby([departure_df['ç¦»å¼€'].dt.date, 'æ¥¼å±‚'])['æˆ¿æ•°'].sum().unstack(fill_value=0)
                        departure_summary.index.name = "ç¦»åº—æ—¥æœŸ"
                        st.dataframe(departure_summary)
                    else:
                        st.warning(f"åœ¨æ‰€é€‰æ—¥æœŸå’ŒçŠ¶æ€å†…æ²¡æœ‰æ‰¾åˆ°ç¦»åº—è®°å½•ã€‚")
                except ValueError:
                    st.error("ç¦»åº—æ—¥æœŸæ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·è¾“å…¥ YYYY/MM/DD æ ¼å¼ã€‚")
            if not arrival_summary.empty or not departure_summary.empty:
                df_to_download = {}
                if not arrival_summary.empty: df_to_download["åˆ°åº—ç»Ÿè®¡"] = arrival_summary
                if not departure_summary.empty: df_to_download["ç¦»åº—ç»Ÿè®¡"] = departure_summary
                excel_data = to_excel(df_to_download)
                st.download_button(label="ä¸‹è½½ç»Ÿè®¡ç»“æœä¸º Excel", data=excel_data, file_name="arrival_departure_summary.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
        st.markdown("---")
        st.header("2. æ¯æ—¥åœ¨ä½æˆ¿é—´æŒ‰ä»·æ ¼åˆ†å¸ƒçŸ©é˜µ")
        with st.expander("ç‚¹å‡»å±•å¼€æˆ–æŠ˜å ", expanded=True):
            default_stay_date = ""
            if not expanded_df.empty and 'ä½åº—æ—¥' in expanded_df.columns:
                   default_stay_date = pd.to_datetime(expanded_df['ä½åº—æ—¥'].min()).strftime('%Y/%m/%d')
            stay_dates_str = st.text_input("è¾“å…¥ä½åº—æ—¥æœŸ (ç”¨é€—å·åˆ†éš”, æ ¼å¼: YYYY/MM/DD)", default_stay_date)
            selected_stay_dates = []
            if stay_dates_str:
                try:
                    stay_date_strings = [d.strip() for d in stay_dates_str.split(',') if d.strip()]
                    selected_stay_dates = [pd.to_datetime(d, format='%Y/%m/%d').date() for d in stay_date_strings]
                except ValueError:
                    st.error("ä½åº—æ—¥æœŸæ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·è¾“å…¥ YYYY/MM/DD æ ¼å¼ã€‚")
                    st.stop()
            all_market_codes = sorted(original_df['å¸‚åœºç '].dropna().unique())
            selected_market_codes = st.multiselect("é€‰æ‹©å¸‚åœºç  (å¯å¤šé€‰)", options=all_market_codes, default=all_market_codes)
            st.subheader("è‡ªå®šä¹‰ä»·æ ¼åŒºé—´")
            col1, col2 = st.columns(2)
            with col1:
                price_bins_jinling_str = st.text_input("é‡‘é™µæ¥¼ä»·æ ¼åŒºé—´", "<401, 401-480, 481-500, 501-550, 551-599, >599")
            with col2:
                price_bins_yatal_str = st.text_input("äºšå¤ªæ¥¼ä»·æ ¼åŒºé—´", "<501, 501-600, 601-699, 700-749, 750-799, >799")
            def parse_price_bins(price_bins_str):
                if not price_bins_str.strip(): return None, None
                intervals = []
                for item in price_bins_str.split(','):
                    item = item.strip()
                    if item.startswith('<'):
                        upper = int(re.search(r'\d+', item).group())
                        intervals.append({'lower': float('-inf'), 'upper': upper, 'label': f'< {upper}'})
                    elif item.startswith('>'):
                        lower = int(re.search(r'\d+', item).group())
                        intervals.append({'lower': lower, 'upper': float('inf'), 'label': f'> {lower}'})
                    elif '-' in item:
                        parts = item.split('-')
                        lower, upper = int(parts[0]), int(parts[1])
                        if lower >= upper: raise ValueError(f"ä»·æ ¼åŒºé—´ '{item}' æ— æ•ˆï¼šä¸‹é™å¿…é¡»å°äºä¸Šé™ã€‚")
                        intervals.append({'lower': lower, 'upper': upper, 'label': f'{lower}-{upper}'})
                    else: raise ValueError(f"æ— æ³•è§£æåŒºé—´ '{item}'")
                intervals.sort(key=lambda x: x['lower'])
                bins = [d['lower'] for d in intervals] + [intervals[-1]['upper']]
                labels = [d['label'] for d in intervals]
                return bins, labels
            try:
                bins_jinling, labels_jinling = parse_price_bins(price_bins_jinling_str)
                bins_yatal, labels_yatal = parse_price_bins(price_bins_yatal_str)
            except (ValueError, IndexError, AttributeError) as e:
                st.error(f"ä»·æ ¼åŒºé—´æ ¼å¼ä¸æ­£ç¡®ã€‚é”™è¯¯: {e}")
                st.stop()
            dfs_to_download_matrix = {}
            if selected_stay_dates and selected_market_codes:
                matrix_df = expanded_df[(expanded_df['ä½åº—æ—¥'].dt.date.isin(selected_stay_dates)) & (expanded_df['å¸‚åœºç '].isin(selected_market_codes))].copy()
                if not matrix_df.empty:
                    buildings = sorted(matrix_df['æ¥¼å±‚'].unique())
                    for building in buildings:
                        st.subheader(f"{building} - åœ¨ä½æˆ¿é—´åˆ†å¸ƒ")
                        building_df = matrix_df[matrix_df['æ¥¼å±‚'] == building]
                        bins, labels = (bins_jinling, labels_jinling) if building == "é‡‘é™µæ¥¼" else (bins_yatal, labels_yatal)
                        if not building_df.empty and bins and labels:
                            building_df['ä»·æ ¼åŒºé—´'] = pd.cut(building_df['æˆ¿ä»·'], bins=bins, labels=labels, right=True, include_lowest=True)
                            pivot_table = pd.pivot_table(building_df.dropna(subset=['ä»·æ ¼åŒºé—´']), index=building_df['ä½åº—æ—¥'].dt.date, columns='ä»·æ ¼åŒºé—´', values='æˆ¿æ•°', aggfunc='sum', fill_value=0)
                            if not pivot_table.empty:
                                pivot_table['æ¯æ—¥æ€»è®¡'] = pivot_table.sum(axis=1)
                                st.dataframe(pivot_table.sort_index())
                                dfs_to_download_matrix[f"{building}_åœ¨ä½åˆ†å¸ƒ"] = pivot_table
                            else:
                               st.info(f"åœ¨ {building} ä¸­ï¼Œæ‰€é€‰æ¡ä»¶ä¸‹çš„æ‰€æœ‰æˆ¿ä»·éƒ½ä¸åœ¨æ‚¨å®šä¹‰çš„ä»·æ ¼åŒºé—´å†…ã€‚")
                        else:
                            st.info(f"åœ¨ {building} ä¸­ï¼Œæ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ‰€é€‰æ¡ä»¶çš„åœ¨ä½è®°å½•æˆ–æœªè®¾ç½®ä»·æ ¼åŒºé—´ã€‚")
                else:
                    st.warning(f"åœ¨æ‰€é€‰æ—¥æœŸå’Œå¸‚åœºç èŒƒå›´å†…æ²¡æœ‰æ‰¾åˆ°åœ¨ä½è®°å½•ã€‚")
            if dfs_to_download_matrix:
                excel_data_matrix = to_excel(dfs_to_download_matrix)
                st.download_button(label="ä¸‹è½½ä»·æ ¼åˆ†å¸ƒçŸ©é˜µä¸º Excel", data=excel_data_matrix, file_name="price_matrix_summary.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
    except Exception as e:
        st.error(f"å¤„ç†æ•°æ®æˆ–ç”ŸæˆæŠ¥å‘Šæ—¶å‘ç”Ÿæ„å¤–é”™è¯¯ã€‚è¯·æ£€æŸ¥æ‚¨çš„Excelæ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®ã€‚")
        st.error(f"æŠ€æœ¯ç»†èŠ‚: {e}")
        st.code(f"Traceback: {traceback.format_exc()}")


# ==============================================================================
# --- [æ–°å¢] APP 5: æ—©ç­è¯æœ¯ç”Ÿæˆå™¨ ---
# ==============================================================================
def run_morning_briefing_app():
    # ... (æ­¤å‡½æ•°ä¿æŒä¸å˜)
    st.title("é‡‘é™µå·¥å…·ç®± - æ—©ç­è¯æœ¯ç”Ÿæˆå™¨")
    st.subheader("æ•°æ®è¾“å…¥")
    col1, col2 = st.columns(2)
    with col1:
        st.markdown("#### é‡‘é™µæ¥¼æ•°æ®")
        jl_occupancy = st.number_input("æ˜¨æ—¥å‡ºç§Ÿç‡ (%)", key="jl_occ", format="%.1f", value=82.4)
        jl_revenue = st.number_input("æ”¶å…¥ (å…ƒ)", key="jl_rev", format="%.1f", value=247173.4)
        jl_adr = st.number_input("å¹³å‡æˆ¿ä»· (å…ƒ)", key="jl_adr", format="%.1f", value=550.5)
        jl_guests = st.number_input("æ€»äººæ•°", key="jl_guests", value=673)
        jl_jinhaiwan = st.number_input("é‡‘æµ·æ¹¾äººæ•°", key="jl_jinhaiwan", value=572)
    with col2:
        st.markdown("#### äºšå¤ªæ¥¼æ•°æ®")
        yt_occupancy = st.number_input("æ˜¨æ—¥å‡ºç§Ÿç‡ (%)", key="yt_occ", format="%.1f", value=83.9)
        yt_revenue = st.number_input("æ”¶å…¥ (å…ƒ)", key="yt_rev", format="%.1f", value=232385.5)
        yt_adr = st.number_input("å¹³å‡æˆ¿ä»· (å…ƒ)", key="yt_adr", format="%.1f", value=719.5)
        yt_guests = st.number_input("æ€»äººæ•°", key="yt_guests", value=485)
        yt_jia = st.number_input("å®¶é¤å…äººæ•°", key="yt_jia", value=323)
    st.markdown("---")
    st.subheader("å…¶ä»–æ•°æ®")
    col3, col4 = st.columns(2)
    with col3:
        onbook_jl = st.number_input("ç›®å‰On Bookå‡ºç§Ÿç‡ - é‡‘é™µæ¥¼ (%)", key="ob_jl", format="%.1f", value=65.5)
        onbook_yt = st.number_input("ç›®å‰On Bookå‡ºç§Ÿç‡ - äºšå¤ªæ¥¼ (%)", key="ob_yt", format="%.1f", value=57.7)
    with col4:
        mini_prog_yesterday = st.number_input("å°ç¨‹åºè®¢æˆ¿ - æ˜¨æ—¥ (é—´å¤œ)", key="mp_yest", value=26)
        mini_prog_today = st.number_input("å°ç¨‹åºè®¢æˆ¿ - ä»Šæ—¥ (é—´å¤œ)", key="mp_today", value=19)
    if st.button("ç”Ÿæˆè¯æœ¯"):
        briefing = (f"æ˜¨æ—¥é‡‘é™µæ¥¼å‡ºç§Ÿç‡{jl_occupancy}%ï¼Œæ”¶å…¥{jl_revenue}å…ƒï¼Œå¹³å‡æˆ¿ä»·{jl_adr}å…ƒï¼Œæ€»äººæ•°{jl_guests}äººï¼Œé‡‘æµ·æ¹¾{jl_jinhaiwan}äººã€‚" f"äºšå¤ªå•†åŠ¡æ¥¼å‡ºç‡{yt_occupancy}%ï¼Œæ”¶å…¥{yt_revenue}å…ƒï¼Œå¹³å‡æˆ¿ä»·{yt_adr}å…ƒï¼Œæ€»äººæ•°{yt_guests}äººï¼Œå®¶é¤å…{yt_jia}äººã€‚" f"ç›®å‰on bookå‡ºç§Ÿç‡é‡‘é™µæ¥¼{onbook_jl}%ï¼Œäºšå¤ªå•†åŠ¡æ¥¼{onbook_yt}%ã€‚" f"å°ç¨‹åºè®¢æˆ¿æ˜¨æ—¥{mini_prog_yesterday}é—´å¤œï¼Œä»Šæ—¥{mini_prog_today}é—´å¤œã€‚")
        st.subheader("ç”Ÿæˆçš„è¯æœ¯")
        st.success(briefing)
        st.code(briefing)

# ==============================================================================
# --- [æ–°å¢] APP 6: å¸¸ç”¨è¯æœ¯å¤åˆ¶å™¨ ---
# ==============================================================================
def run_common_phrases_app():
    # ... (æ­¤å‡½æ•°ä¿æŒä¸å˜)
    st.title("é‡‘é™µå·¥å…·ç®± - å¸¸ç”¨è¯æœ¯")
    phrases = ["CA RM TO CREDIT FM", "å…é¢„ä»˜,æˆ¿è´¹åŠ3000å…ƒä»¥å†…æ‚è´¹è½¬æ·˜å® FM", "æˆ¿è´¹è½¬æºç¨‹å®ç¿ FM", "æˆ¿ä»·ä¿å¯†,æˆ¿è´¹è½¬åä¸º FM", "æˆ¿è´¹è½¬æ·˜å® FM", "CA RM TO å…°è‰³(109789242)é‡‘é™µå¡ FM", "CA RM TO AGODA FM", "CA RM TO CREDIT CARD FM XX-XX/XX(å¡å·/æœ‰æ•ˆæœŸXX/XX)", "æˆ¿è´¹è½¬å¾®ä¿¡ FM", "æˆ¿è´¹é¢„ä»˜æ‚è´¹è‡ªç†FM"]
    st.subheader("ç‚¹å‡»å³ä¸Šè§’å¤åˆ¶å›¾æ ‡å³å¯å¤åˆ¶è¯æœ¯")
    for phrase in phrases:
        st.code(phrase, language=None)


# ==============================================================================
# --- [é‡æ„] APP 7: é¢„ç®—è®¡ç®—å™¨ (è¡¨æ ¼ç‰ˆ V2) ---
# ==============================================================================
def run_budget_calculator_app():
    st.title("é‡‘é™µå·¥å…·ç®± - é¢„ç®—è®¡ç®—å™¨")
    st.info("è¯·åœ¨ä¸‹æ–¹è¡¨æ ¼ä¸­è¾“å…¥æ¯æ—¥çš„é¢„è®¡å’Œå®é™…æ•°æ®ï¼Œç„¶åç‚¹å‡»è®¡ç®—æŒ‰é’®ã€‚")

    st.subheader("æ•°æ®è¾“å…¥")
    
    today = date.today()
    days = [(today + timedelta(days=i)) for i in range(7)]
    weekdays_zh = ["å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥"]
    
    initial_data = {
        "æ—¥æœŸ": [d.strftime("%m/%d") for d in days],
        "æ˜ŸæœŸ": [weekdays_zh[d.weekday()] for d in days],
        "å½“æ—¥é¢„è®¡": [0.0] * 7,
        "å½“æ—¥å®é™…": [0.0] * 7,
        "å‘¨ä¸€é¢„è®¡": [0.0] * 7,
        "å¹³å‡æˆ¿ä»·": [0.0] * 7
    }
    input_df = pd.DataFrame(initial_data)
    
    st.info("è¯´æ˜ï¼š'å‘¨ä¸€é¢„è®¡'åˆ—é€šå¸¸åªéœ€è¦åœ¨ç¬¬ä¸€è¡Œï¼ˆå‘¨ä¸€ï¼‰å¡«å†™ï¼Œè®¡ç®—æ—¶ä¼šè‡ªåŠ¨åº”ç”¨åˆ°åç»­æ—¥æœŸã€‚")

    edited_df = st.data_editor(
        input_df, 
        key="budget_editor", 
        num_rows="fixed",
        use_container_width=True,
        column_config={
            "å½“æ—¥é¢„è®¡": st.column_config.NumberColumn(format="%.2f"),
            "å½“æ—¥å®é™…": st.column_config.NumberColumn(format="%.2f"),
            "å‘¨ä¸€é¢„è®¡": st.column_config.NumberColumn(format="%.2f"),
            "å¹³å‡æˆ¿ä»·": st.column_config.NumberColumn(format="%.2f"),
        }
    )

    st.subheader("è®¡ç®—ç»“æœ")
    if st.button("è®¡ç®—å¹¶ç”ŸæˆæŠ¥å‘Š"):
        try:
            result_df = edited_df.copy()
            numeric_cols = ["å½“æ—¥é¢„è®¡", "å½“æ—¥å®é™…", "å‘¨ä¸€é¢„è®¡", "å¹³å‡æˆ¿ä»·"]
            for col in numeric_cols:
                result_df[col] = pd.to_numeric(result_df[col], errors='coerce').fillna(0)
            
            monday_forecast_val = result_df['å‘¨ä¸€é¢„è®¡'].iloc[0]
            if monday_forecast_val != 0:
                result_df['å‘¨ä¸€é¢„è®¡'] = monday_forecast_val

            # [å…³é”®ä¿®æ”¹] æŒ‰ç…§ç”¨æˆ·è¦æ±‚çš„é€»è¾‘è¿›è¡Œè®¡ç®—
            result_df["å½“æ—¥å¢åŠ ç‡"] = result_df["å½“æ—¥å®é™…"] - result_df["å½“æ—¥é¢„è®¡"]
            result_df["å¢åŠ ç™¾åˆ†ç‡"] = result_df["å½“æ—¥å®é™…"] - result_df["å‘¨ä¸€é¢„è®¡"]
            
            # [å…³é”®ä¿®æ”¹] è°ƒæ•´åˆ—é¡ºåºä»¥åŒ¹é…Excelè¡¨æ ¼çš„é€»è¾‘åˆ†ç»„
            display_columns = [
                "æ—¥æœŸ", "æ˜ŸæœŸ", 
                "å½“æ—¥é¢„è®¡", "å½“æ—¥å®é™…", "å½“æ—¥å¢åŠ ç‡", 
                "å‘¨ä¸€é¢„è®¡", "å¢åŠ ç™¾åˆ†ç‡", "å¹³å‡æˆ¿ä»·"
            ]
            result_df_display = result_df[display_columns].copy()
            
            # ä¸ºäº†åœ¨è¡¨æ ¼ä¸­æ¸…æ™°åœ°å±•ç¤ºâ€œå‘¨ä¸€é¢„è®¡â€å’Œâ€œå½“æ—¥å®é™…â€çš„å¯¹æ¯”ï¼Œæˆ‘ä»¬å¯ä»¥æ’å…¥ä¸€åˆ—
            result_df_display.insert(6, 'å½“æ—¥å®é™…(ç”¨äºå‘¨æ¯”)', result_df['å½“æ—¥å®é™…'])

            st.dataframe(result_df_display.style.format({
                "å½“æ—¥é¢„è®¡": "{:.2f}", "å½“æ—¥å®é™…": "{:.2f}", "å½“æ—¥å¢åŠ ç‡": "{:+.2f}",
                "å‘¨ä¸€é¢„è®¡": "{:.2f}", 'å½“æ—¥å®é™…(ç”¨äºå‘¨æ¯”)': "{:.2f}", "å¢åŠ ç™¾åˆ†ç‡": "{:+.2f}", "å¹³å‡æˆ¿ä»·": "{:.2f}"
            }))
            
            st.markdown("---")
            st.subheader("æœ¬å‘¨æ€»è®¡")
            
            total_actual = result_df['å½“æ—¥å®é™…'].sum()
            total_forecast = result_df['å½“æ—¥é¢„è®¡'].sum()
            total_increase = total_actual - total_forecast
            
            col1, col2, col3 = st.columns(3)
            col1.metric("æœ¬å‘¨å®é™…", f"{total_actual:.2f}")
            col2.metric("æœ¬å‘¨é¢„æµ‹", f"{total_forecast:.2f}")
            col3.metric("å®é™…å¢åŠ ", f"{total_increase:+.2f}")

        except (ValueError, IndexError, KeyError) as e:
            st.error(f"è®¡ç®—æ—¶å‘ç”Ÿé”™è¯¯ï¼Œè¯·æ£€æŸ¥è¾“å…¥çš„æ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®ã€‚é”™è¯¯è¯¦æƒ…: {e}")

# ==============================================================================
# --- å…¨å±€å‡½æ•°å’Œä¸»åº”ç”¨è·¯ç”±å™¨ ---
# ==============================================================================
@st.cache_data
def to_excel(df_dict):
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        for sheet_name, df in df_dict.items():
            df.to_excel(writer, sheet_name=sheet_name)
    processed_data = output.getvalue()
    return processed_data
    
def check_password():
    """è¿”å› True å¦‚æœç”¨æˆ·å·²ç™»å½•, å¦åˆ™è¿”å› False."""
    def login_form():
        with st.form("Credentials"):
            st.text_input("ç”¨æˆ·å", key="username")
            st.text_input("å¯†ç ", type="password", key="password")
            st.form_submit_button("ç™»å½•", on_click=password_entered)

    def password_entered():
        app_username = st.secrets.app_credentials.get("username")
        app_password = st.secrets.app_credentials.get("password")
        if st.session_state["username"] == app_username and st.session_state["password"] == app_password:
            st.session_state["password_correct"] = True
            if "password" in st.session_state: del st.session_state["password"]
            if "username" in st.session_state: del st.session_state["username"]
        else:
            st.session_state["password_correct"] = False

    if "app_credentials" not in st.secrets or not st.secrets.app_credentials.get("username") or not st.secrets.app_credentials.get("password"):
        st.error("é”™è¯¯ï¼šåº”ç”¨çš„ç”¨æˆ·åæˆ–å¯†ç æœªåœ¨ Streamlit Secrets ä¸­æ­£ç¡®é…ç½®ã€‚")
        return False

    if st.session_state.get("password_correct", False):
        return True

    login_form()
    if "password_correct" in st.session_state and not st.session_state.password_correct:
        st.error("ç”¨æˆ·åæˆ–å¯†ç ä¸æ­£ç¡®ã€‚")
    return False


# --- ä¸»åº”ç”¨è·¯ç”±å™¨ ---
st.set_page_config(layout="wide", page_title="é‡‘é™µå·¥å…·ç®±")

if check_password():
    with st.sidebar:
        app_choice = option_menu(
            menu_title="é‡‘é™µå·¥å…·ç®±",
            options=["OCR å·¥å…·", "é¢„ç®—è®¡ç®—å™¨", "æ¯”å¯¹å¹³å°", "å›¢é˜Ÿåˆ°åº—ç»Ÿè®¡", "æ•°æ®åˆ†æ", "è¯æœ¯ç”Ÿæˆå™¨", "å¸¸ç”¨è¯æœ¯"],
            icons=["camera-reels-fill", "calculator", "kanban", "clipboard-data", "graph-up-arrow", "blockquote-left", "card-text"],
            menu_icon="tools",
            default_index=0,
        )

    st.sidebar.markdown("---")
    st.sidebar.info("è¿™æ˜¯ä¸€ä¸ªå°†å¤šä¸ªå·¥å…·é›†æˆåˆ°ä¸€èµ·çš„åº”ç”¨ã€‚")

    if app_choice == "OCR å·¥å…·":
        run_ocr_app_detailed()
    elif app_choice == "é¢„ç®—è®¡ç®—å™¨":
        run_budget_calculator_app()
    elif app_choice == "æ¯”å¯¹å¹³å°":
        run_comparison_app()
    elif app_choice == "å›¢é˜Ÿåˆ°åº—ç»Ÿè®¡":
        run_analyzer_app()
    elif app_choice == "æ•°æ®åˆ†æ":
        run_data_analysis_app()
    elif app_choice == "è¯æœ¯ç”Ÿæˆå™¨":
        run_morning_briefing_app()
    elif app_choice == "å¸¸ç”¨è¯æœ¯":
        run_common_phrases_app()

